{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQjEulbLIc7WN6zt5ZfNOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuyangAbert/DeepALCS/blob/main/DeepAL2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cp-h5ErvVk0",
        "outputId": "27474ed4-321a-417c-ddb5-c222432369ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist,squareform\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# from featureselection import SFSFC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import f1_score,precision_score,auc, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from math import exp\n",
        "import numpy.matlib\n",
        "# import tensorflow.compat.v1 as tf\n",
        "import tensorflow as tf\n",
        "# from keras.applications.resnet50     import ResNet50\n",
        "# from keras.applications.vgg16        import VGG16\n",
        "# from keras.applications.vgg19        import VGG19\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "# from keras.optimizers import SGD, Adam\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Input, Dense, AveragePooling2D, GlobalAveragePooling2D, MaxPool2D\n",
        "from keras.applications.inception_v3 import preprocess_input as incv3_preprocess_input\n",
        "# from keras.applications.resnet50     import preprocess_input as resnet50_preprocess_input\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10, cifar100,fashion_mnist\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.applications.vgg16        import preprocess_input as vgg16_preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "# from livelossplot.inputs.keras import PlotLossesCallback"
      ],
      "metadata": {
        "id": "VWlYJpTGwJCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Pre_Data(data):\n",
        "    [N, L] = np.shape(data)\n",
        "    NewData = np.empty((N, L))\n",
        "    for i in range(L):\n",
        "        Temp = data[:, i]\n",
        "        NewData[:, i] = Temp\n",
        "    return NewData\n",
        "\n",
        "\n",
        "def ParamSpe(data):\n",
        "    Buffersize = 1000\n",
        "    PreStd = []\n",
        "    P_Summary = []\n",
        "    PFS = []\n",
        "    T = round(np.shape(data)[0] / Buffersize)\n",
        "    return Buffersize, P_Summary, T, PFS, PreStd\n",
        "\n",
        "\n",
        "def FeatureType(data):\n",
        "    [N, dim] = np.shape(data)\n",
        "    th = round(N ** 0.5)\n",
        "    F_cont = []\n",
        "    F_disc = []\n",
        "    F_zero = []\n",
        "    for j in range(dim):\n",
        "        temp_unique = np.unique(data[:, j])\n",
        "        if len(temp_unique) > th:\n",
        "            F_cont.append(j)\n",
        "        elif len(temp_unique) > 1:\n",
        "            F_disc.append(j)\n",
        "        if len(temp_unique) == 1:\n",
        "            F_zero.append(j)\n",
        "    return F_cont, F_disc, F_zero\n",
        "\n",
        "\n",
        "def Distance_Cal(data):\n",
        "    D = pdist(data)\n",
        "    Dist = squareform(D)\n",
        "    return Dist\n",
        "\n",
        "\n",
        "def Fitness_Cal(sample, pop, stdData, gamma):\n",
        "    Ns = np.shape(sample)[0]\n",
        "    Np = np.shape(pop)[0]\n",
        "    Newsample = np.concatenate([sample, pop])\n",
        "    Dist = Distance_Cal(Newsample)\n",
        "    fitness = []\n",
        "    for i in range(Np):\n",
        "        distArray = np.power(Dist[i + Ns, 0:Ns], 2)\n",
        "        temp = np.power(np.exp(-distArray / stdData), gamma)\n",
        "        fitness.append(np.sum(temp))\n",
        "    return fitness\n",
        "\n",
        "\n",
        "def fitness_update(P_Summary, Current, fitness, PreStd, gamma, stdData):\n",
        "    [N, dim] = np.shape(Current)\n",
        "    t_I = len(PreStd)\n",
        "    NewFit = fitness\n",
        "    if len(P_Summary) > 0:\n",
        "        PreFit = P_Summary[:, dim]\n",
        "        PreP = P_Summary[:, 0:dim]\n",
        "        OldStd = PreStd[t_I - 1]\n",
        "        for i in range(N):\n",
        "            fitin = 0\n",
        "            for j in range(np.shape(PreP)[0]):\n",
        "                if np.linalg.norm(Current[i][:] - PreP[j][:]) < 0.01:\n",
        "                    fitin = PreFit[j]\n",
        "                    break\n",
        "                else:\n",
        "                    d = np.linalg.norm(Current[i][:] - PreP[j][:])\n",
        "                    fitin += (exp(-d ** 2 / stdData) ** gamma) * (PreFit[j] ** (OldStd / stdData))\n",
        "            NewFit[i] = fitness[i] + fitin\n",
        "    return NewFit\n",
        "\n",
        "\n",
        "def PopInitial(sample, PreMu, PreStd, Buffersize):\n",
        "    [N, L] = np.shape(sample)\n",
        "    pop_Size = round(1 * N)\n",
        "    # Compute the statistics of the current data chunk\n",
        "    minLimit = np.min(sample, axis=0)\n",
        "    meanData = np.mean(sample, axis=0)\n",
        "    maxLimit = np.max(sample, axis=0)\n",
        "    # Update the statistics of the data stream\n",
        "    meanData = UpdateMean(PreMu, meanData, Buffersize)\n",
        "    PreMu.append(meanData)\n",
        "    # Compute the standard deviation of the current data chunk\n",
        "    MD = np.matlib.repmat(meanData, N, 1)\n",
        "    tempSum = np.sum(np.sum((MD - sample) ** 2, axis=1))\n",
        "    stdData = tempSum / N\n",
        "    # Update the standard deviation of the data stream\n",
        "    stdData = StdUpdate(stdData, PreStd, Buffersize)\n",
        "    # Randonmly Initialize the population indices from the data chunk\n",
        "    pop_Index = np.arange(0, N)\n",
        "    pop = sample[pop_Index, :]\n",
        "    # Calculate the initial niche radius\n",
        "    radius = numpy.linalg.norm((maxLimit - minLimit)) * 0.4  # 0.6\n",
        "\n",
        "    return [stdData, pop_Index, pop, radius, PreMu, PreStd]\n",
        "\n",
        "\n",
        "def UpdateMean(PreMy, meanData, BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreMu)\n",
        "    # Update the mean of the data stream as new data chunk arrives\n",
        "    if t_P == 0:\n",
        "        newMu = meanData\n",
        "    else:\n",
        "        oldMu = PreMu[t_P - 1][:]\n",
        "        newMu = (meanData + oldMu * t_P) / (t_P + 1)\n",
        "    return newMu\n",
        "\n",
        "\n",
        "def StdUpdate(Std, PreStd, BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreStd)\n",
        "    # Update the variance of the data stream as new data chunk arrives\n",
        "    if t_P == 0:\n",
        "        newStd = Std\n",
        "    else:\n",
        "        oldStd = PreStd[t_P - 1]\n",
        "        newStd = (Std + oldStd * t_P) / (t_P + 1)\n",
        "    return newStd\n",
        "\n",
        "\n",
        "# ------------------------Parameter Estimation----------------------------#\n",
        "def CCA(sample, stdData, Dist):\n",
        "    m = 1\n",
        "    gamma = 5\n",
        "    ep = 0.998\n",
        "    N = np.shape(sample)[0]\n",
        "    while 1:\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N - 1):\n",
        "            Diff = np.power(Dist[i, :], 2)\n",
        "            temp1 = np.power(np.exp(-Diff / stdData), gamma * m)\n",
        "            temp2 = np.power(np.exp(-Diff / stdData), gamma * (m + 1))\n",
        "            den1.append(np.sum(temp1))\n",
        "            den2.append(np.sum(temp2))\n",
        "\n",
        "        y = np.corrcoef(den1, den2)[0, 1]\n",
        "\n",
        "        if y > ep:\n",
        "            break\n",
        "        m = m + 1\n",
        "    return m * gamma\n",
        "\n",
        "\n",
        "def DCCA(sample, stdData, P_Summary, gamma, dim):\n",
        "    P_Center = P_Summary[:, 0:dim]\n",
        "    P_F = P_Summary[:, dim]\n",
        "    gam1 = gamma\n",
        "    N1 = np.shape(sample)[0]\n",
        "    N2 = np.shape(P_Center)[0]\n",
        "    ep = 0.998\n",
        "    N = N1 + N2\n",
        "    temp = np.concatenate([sample, P_Center], axis=0)\n",
        "    Dist = Distance_Cal(temp)\n",
        "    while 1:\n",
        "        gam2 = gam1 + 5\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N):\n",
        "            Diff = np.power(Dist[i, 0:N1], 2)\n",
        "            temp1 = np.power(np.exp(-Diff / stdData), gam1)\n",
        "            temp2 = np.power(np.exp(-Diff / stdData), gam2)\n",
        "            sum1 = np.sum(temp1)\n",
        "            sum2 = np.sum(temp2)\n",
        "            if i < N1:\n",
        "                T1 = 0\n",
        "                T2 = 0\n",
        "                for j in range(N2):\n",
        "                    T1 += P_F[j] ** (gam1 / gamma)\n",
        "                    T2 += P_F[j] ** (gam2 / gamma)\n",
        "                s1 = sum1 + T1\n",
        "                s2 = sum2 + T2\n",
        "            #                s1 = sum1**(gam1/gamma) + T1\n",
        "            #                s2 = sum2**(gam2/gamma) + T2\n",
        "            else:\n",
        "                #                s1 = sum1**(gam1/gamma) + P_F[i-N1]**(gam1/gamma)\n",
        "                #                s2 = sum2**(gam2/gamma) + P_F[i-N1]**(gam2/gamma)\n",
        "                s1 = sum1 + P_F[i - N1] ** (gam1 / gamma)\n",
        "                s2 = sum2 + P_F[i - N1] ** (gam2 / gamma)\n",
        "            den1.append(s1)\n",
        "            den2.append(s2)\n",
        "        y = np.corrcoef(den1, den2)[0, 1]\n",
        "        if y > ep:\n",
        "            break\n",
        "        gam1 = gam2\n",
        "    return gam1\n",
        "\n",
        "\n",
        "def TPC_Search(Dist, Pop_Index, Pop, radius, fitness):\n",
        "    # Extract the size of the population\n",
        "    [N, dim] = np.shape(Pop)\n",
        "    P = []  # Initialize the Peak Vector\n",
        "    P_fitness = []\n",
        "    # i = 1\n",
        "    marked = []\n",
        "    co = []\n",
        "    OriginalIndice = Pop_Index\n",
        "    while 1:\n",
        "        # -------------Search for the local maximum-----------------#\n",
        "        SortIndice = np.argsort(fitness)\n",
        "        NewIndice = SortIndice[::-1]\n",
        "\n",
        "        Pop = Pop[NewIndice, :]\n",
        "        fitness = fitness[NewIndice]\n",
        "        OriginalIndice = OriginalIndice[NewIndice]\n",
        "\n",
        "        P.append(Pop[0, :])\n",
        "\n",
        "        P_fitness.append(fitness[0])\n",
        "        P_Indice = OriginalIndice[0]\n",
        "\n",
        "        Ind = AssigntoPeaks(Pop, Pop_Index, P, P_Indice, marked, radius, Dist)\n",
        "\n",
        "        marked.append(Ind)\n",
        "        marked.append(NewIndice[0])\n",
        "\n",
        "        if not Ind:\n",
        "            Ind = [NewIndice[0]]\n",
        "\n",
        "        co.append(len(Ind))\n",
        "        TempFit = fitness\n",
        "        sum1 = 0\n",
        "        for j in range(len(Ind)):\n",
        "            sum1 += fitness[np.where(OriginalIndice == Ind[j])]\n",
        "        for th in range(len(Ind)):\n",
        "            TempFit[np.where(OriginalIndice == Ind[th])] = fitness[np.where(OriginalIndice == Ind[th])] / sum1\n",
        "        fitness = TempFit\n",
        "        if np.sum(co) >= N:\n",
        "            P = np.asarray(P)\n",
        "            P_fitness = np.asarray(P_fitness)\n",
        "            break\n",
        "\n",
        "    return P, P_fitness\n",
        "\n",
        "\n",
        "def MergeInChunk(P, P_fitness, sample, gamma, stdData):\n",
        "    \"\"\"Perform the Merge of TPCs witnin each data chunk\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc, dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked = []\n",
        "    unmarked = []\n",
        "    Com = []\n",
        "\n",
        "    # Num of TPCs\n",
        "    Nc = np.shape(P)[0]\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j != i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j, :] - P[i, :])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice <= Nc:\n",
        "                MinIndice = int(MinIndice)\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i, :]) / 2\n",
        "\n",
        "                X = np.reshape(X, (1, np.shape(P)[1]))\n",
        "\n",
        "                fitX = Fitness_Cal(sample, X, stdData, gamma)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 0.85 * min(fitN, fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i, MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k, 0]] >= P_fitness[Com[k, 1]]:\n",
        "            NewP.append(P[Com[k, 0], :])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k, 1], :])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n, :])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP, NewP_fitness\n",
        "\n",
        "\n",
        "def MergeOnline(P, P_fitness, P_summary, PreStd, sample, gamma, stdData):\n",
        "    \"\"\"Perform the Merge of Clusters Between Historical and New Clusters\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc, dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked = []\n",
        "    unmarked = []\n",
        "    Com = []\n",
        "\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j != i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j, :] - P[i, :])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice < Nc:\n",
        "\n",
        "                #                MinIndice = int(MinIndice)\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i][:]) / 2\n",
        "                X = np.reshape(X, (1, np.shape(P)[1]))\n",
        "                RfitX = Fitness_Cal(sample, X, stdData, gamma)\n",
        "                fitX = fitness_update(P_Summary, X, RfitX, PreStd, gamma, stdData)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 0.85 * min(fitN, fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i, MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k, 0]] >= P_fitness[Com[k, 1]]:\n",
        "            NewP.append(P[Com[k, 0]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k, 1]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n][:])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP, NewP_fitness\n",
        "\n",
        "\n",
        "def CE_InChunk(sample, P, P_fitness, stdData, gamma):\n",
        "    while 1:\n",
        "        HistP = P\n",
        "        #        HistPF = P_fitness\n",
        "        P, P_fitness = MergeInChunk(P, P_fitness, sample, gamma, stdData)\n",
        "        if np.shape(P)[0] == np.shape(HistP)[0]:\n",
        "            break\n",
        "    return P, P_fitness\n",
        "\n",
        "\n",
        "def CE_Online(sample, P_Summary, P, P_fitness, stdData, gamma, PreStd):\n",
        "    dim = np.shape(P)[1]\n",
        "\n",
        "    # Concatenate the historical and new clusters together\n",
        "    PC = np.concatenate([P_Summary[:, 0:dim], P])\n",
        "    RPF = Fitness_Cal(sample, PC, stdData, gamma)\n",
        "    PF = fitness_update(P_Summary, PC, RPF, PreStd, gamma, stdData)\n",
        "\n",
        "    while 1:\n",
        "        HistPC = PC\n",
        "        #        HistPF = PF\n",
        "        PC, PF = MergeOnline(PC, PF, P_Summary, PreStd, sample, gamma, stdData)\n",
        "        RPF = Fitness_Cal(sample, PC, stdData, gamma)\n",
        "        PF = fitness_update(P_Summary, PC, RPF, PreStd, gamma, stdData)\n",
        "        if np.shape(PC)[0] == np.shape(HistPC)[0]:\n",
        "            break\n",
        "    return PC, PF\n",
        "\n",
        "\n",
        "def ClusterValidation(sample, P):\n",
        "    while 1:\n",
        "        NewP = []\n",
        "        PreP = P\n",
        "        [R_d, RIndice] = Cluster_Assign(sample, P)\n",
        "\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            Temp = np.where(RIndice == i)\n",
        "            Temp = np.asarray(Temp)\n",
        "            if np.shape(Temp)[1] > 2:\n",
        "                NewP.append(P[i][:])\n",
        "        P = NewP\n",
        "        if np.shape(P)[0] == np.shape(PreP)[0]:\n",
        "            break\n",
        "    return np.asarray(P)\n",
        "\n",
        "\n",
        "def ClusterSummary(P, PF, P_Summary, sample):\n",
        "    dim = np.shape(sample)[1]\n",
        "    Rp = AverageDist(P, P_Summary, sample, dim)\n",
        "    P = np.asarray(P)\n",
        "    PF = [PF]\n",
        "\n",
        "    PF = np.asarray(PF)\n",
        "    Rp = np.reshape(Rp, (np.shape(P)[0], 1))\n",
        "    PCluster = np.concatenate([P, PF.T], axis=1)\n",
        "    PCluster = np.concatenate([PCluster, Rp], axis=1)\n",
        "\n",
        "    P_Summary = PCluster\n",
        "\n",
        "    return P_Summary\n",
        "\n",
        "\n",
        "def StoreInf(PF, PFS, PreStd, stdData):\n",
        "    PreStd.append(stdData)\n",
        "    PFS.append(PF)\n",
        "    return PreStd, PFS\n",
        "\n",
        "\n",
        "# --------------------Cluster Radius Computation and Update--------------------#\n",
        "def AverageDist(P, P_Summary, sample, dim):\n",
        "    P = P\n",
        "    # Obtain the assignment of clusters\n",
        "    [distance, indices] = Cluster_Assign(sample, P)\n",
        "    rad1 = []\n",
        "    # if the summary of clusters is not empty\n",
        "    if len(P_Summary) > 0:\n",
        "\n",
        "        PreP = P_Summary[:, 0:dim]  # Hstorical Cluster Center vector\n",
        "        PreR = P_Summary[:, dim + 1]\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            if np.shape(np.where(indices == i))[1] > 1:\n",
        "                SumD1 = 0\n",
        "                Count1 = 0\n",
        "                for j in range(np.shape(sample)[0]):\n",
        "                    if indices[j] == i:\n",
        "                        SumD1 += distance[j]\n",
        "                        Count1 += 1\n",
        "                rad1.append(SumD1 / Count1)\n",
        "            else:\n",
        "                C_d = []\n",
        "                for k in range(np.shape(PreP)[0]):\n",
        "                    C_d.append(np.linalg.norm(P[i][:] - PreP[k][:]))\n",
        "                CI = np.argmin(C_d)\n",
        "                rad1.append(PreR[CI])\n",
        "    elif not P_Summary:\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            SumD1 = 0\n",
        "            Count1 = 0\n",
        "            for j in range(np.shape(sample)[0]):\n",
        "                if indices[j] == i:\n",
        "                    SumD1 += distance[j]\n",
        "                    Count1 += 1\n",
        "            rad1.append(SumD1 / Count1)\n",
        "    return np.asarray(rad1)\n",
        "\n",
        "\n",
        "def AssigntoPeaks(pop, pop_index, P, P_I, marked, radius, Dist):\n",
        "    temp = []\n",
        "    [N, L] = np.shape(pop)\n",
        "    for i in range(N):\n",
        "        distance = Dist[i, P_I]\n",
        "        if not np.any(marked == pop_index[i]):\n",
        "            if distance < radius:\n",
        "                temp.append(pop_index[i])\n",
        "    indices = temp\n",
        "    return indices\n",
        "\n",
        "\n",
        "def Cluster_Assign(sample, P):\n",
        "    # Number of samples\n",
        "    N = np.shape(sample)[0]\n",
        "    # Number of Clusters at t\n",
        "    Np = np.shape(P)[0]\n",
        "    MinDist = []\n",
        "    MinIndice = []\n",
        "    for i in range(N):\n",
        "        d = []\n",
        "        for j in range(Np):\n",
        "            d.append(np.linalg.norm(sample[i][:] - P[j][:]))\n",
        "        if len(d) <= 1:\n",
        "            tempD = d\n",
        "            tempI = 0\n",
        "        else:\n",
        "            tempD = np.min(d)\n",
        "            tempI = np.argmin(d)\n",
        "\n",
        "        MinDist.append(tempD)\n",
        "        MinIndice.append(tempI)\n",
        "    MinDist = np.asarray(MinDist)\n",
        "    MinIndice = np.asarray(MinIndice)\n",
        "    return MinDist, MinIndice\n",
        "def compute_radius(MinDist,ClusterIndice):\n",
        "    cluster = np.unique(ClusterIndice)\n",
        "    nc = len(cluster)\n",
        "    cluster_rad = []\n",
        "    for i in range(nc):\n",
        "        currentcluster = np.where(ClusterIndice==cluster[i])[0]\n",
        "        cluster_rad.append(np.mean(MinDist[currentcluster]))\n",
        "    return cluster_rad"
      ],
      "metadata": {
        "id": "d6Z1Xg9F7BQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering Procedure based on FPS-clustering algorithm**"
      ],
      "metadata": {
        "id": "SdvTBex2TwM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------Main Function-------------------------#\n",
        "if __name__ == '__main__':\n",
        "    start = time.time()\n",
        "    (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "    # (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    # data = X_train.reshape(X_train.shape[0], 3072)\n",
        "    # label = y_train\n",
        "\n",
        "    # dim = np.shape(data)[1]\n",
        "    # [BufferSize,P_Summary,T,PFS,PreStd] = ParamSpe(data)\n",
        "    # T = int(T)\n",
        "    # gammaHist = []\n",
        "    # PFS = []\n",
        "    # PreMu = []\n",
        "\n",
        "    # for t in range(T):\n",
        "    #     if t < T-1:\n",
        "    #        sample = data[t*BufferSize:(t+1)*BufferSize,:]\n",
        "    #     else:\n",
        "    #         sample = data[t*BufferSize:np.shape(data)[0]]\n",
        "    #     if t==0:\n",
        "    #         AccSample = sample\n",
        "    #     else:\n",
        "    #         AccSample = np.concatenate([AccSample,sample])\n",
        "\n",
        "    #     [stdData,pop_index,pop,radius,PreMu,PreStd] = PopInitial(sample,PreMu,PreStd,BufferSize)\n",
        "    #     # Initialize the fitness vector\n",
        "    #     fitness = np.zeros((len(pop_index),1))\n",
        "    #     # Initialize the indices vector\n",
        "    #     indices = np.zeros((len(pop_index),1))\n",
        "    #     Dist = Distance_Cal(sample)\n",
        "    #     if PreStd:\n",
        "    #         if PreStd[len(PreStd)-1] > stdData:\n",
        "    #             P = P_Summary[:,0:dim]\n",
        "    #             localFit = Fitness_Cal(sample,P,stdData,gamma)\n",
        "    #             PF = fitness_update(P_Summary,P,localFit,PreStd,gamma,stdData)\n",
        "    #             P_Summary = ClusterSummary(P,PF,P_Summary,sample)\n",
        "    #             PFS.append(PF)\n",
        "    #             PreStd.append(stdData)\n",
        "    #             clustercenter = P\n",
        "    #             [Assign,clusterindex] = Cluster_Assign(AccSample,P)\n",
        "    #             continue\n",
        "    #     else:\n",
        "    #         gamma = CCA(sample,stdData,Dist)\n",
        "    #     gammaHist.append(gamma)\n",
        "    #     fitness = Fitness_Cal(sample,pop,stdData,gamma)\n",
        "    #     fitness = np.array(fitness)\n",
        "    #     P, P_fitness = TPC_Search(Dist,pop_index,pop,radius,fitness)\n",
        "    #     P, P_fitness = CE_InChunk(sample,P,P_fitness,stdData,gamma)\n",
        "    #     P_fitness = Fitness_Cal(sample,P,stdData,gamma)\n",
        "    #     P_fitness = fitness_update(P_Summary,P,P_fitness,PreStd,gamma,stdData)\n",
        "    #     print('Processing Data Chunk '+str(t))\n",
        "    #     if t == 0:\n",
        "    #         P = P\n",
        "    #         PF = np.asarray(P_fitness)\n",
        "    #     else:\n",
        "    #         P,P_fitness = CE_Online(sample,P_Summary,P,P_fitness,stdData,gamma,PreStd)\n",
        "    #         PF = np.asarray(P_fitness)\n",
        "    #     P_Summary = ClusterSummary(P,PF,P_Summary,sample)\n",
        "    #     PreStd,PFS = StoreInf(PF,PFS,PreStd,stdData)\n",
        "    # # Clustering procedure finishes\n",
        "    # [MinDist,ClusterIndice] = Cluster_Assign(AccSample,P)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcR9qI_I7OWA",
        "outputId": "428453d6-7b24-498b-a79c-a4ee7f8ad01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 13s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Active Learning Process**"
      ],
      "metadata": {
        "id": "075ktR4gTjBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Start Active Learning\n",
        "# sample_size = np.shape(AccSample)[0]\n",
        "# num_S = round(0.4 * sample_size)\n",
        "# cluster_radius = compute_radius(MinDist, ClusterIndice)\n",
        "# delta = np.mean(cluster_radius)\n",
        "# sigma = np.std(cluster_radius)\n",
        "# dense_idx = np.where(cluster_radius <= abs(delta - sigma))[0]\n",
        "# sparese_idx = np.where(cluster_radius > abs(delta - sigma))[0]\n",
        "\n",
        "# FetchIndex = []\n",
        "# UnlabeledIndex = []\n",
        "# InterDist = squareform(pdist(P))\n",
        "\n",
        "# FetchIndex = []\n",
        "# UnlabeledIndex = []\n",
        "# InterDist = squareform(pdist(P))\n",
        "\n",
        "# for i in range(np.shape(P)[0]):\n",
        "#   tempcluster = np.where(ClusterIndice == (i))\n",
        "#   d1 = []\n",
        "#   if i in dense_idx:\n",
        "#     for j in range(len(tempcluster[0])):\n",
        "#       d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "#     fetchSize = round(num_S * cluster_radius[i] / (np.sum(cluster_radius)))\n",
        "#     fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "#     sortIndex1 = np.argsort(d1)\n",
        "#     fet1 = tempcluster[0][sortIndex1[:int(round(fetchSize * 1))]]\n",
        "#     fet1 = fet1.astype(int)\n",
        "#     fet2 = []\n",
        "#     FetchIndex = np.append(FetchIndex, fet1)\n",
        "#     FetchIndex = np.append(FetchIndex, fet2)\n",
        "#     UnlabeledIndex = np.append(UnlabeledIndex,tempcluster[0][sortIndex1[int(round(fetchSize * 1)):round(len(d1))]])\n",
        "#   else:\n",
        "#       temp_interdist = InterDist[i, :]\n",
        "#       temp_rank = np.argsort(temp_interdist)\n",
        "#       temp_neigh1 = P[temp_rank[0], :]\n",
        "#       temp_neigh2 = P[temp_rank[1], :]\n",
        "#       d1 = []\n",
        "#       for j in range(len(tempcluster[0])):\n",
        "#         d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "#       # fetchSize = round(num_S * cluster_radius[i] / np.sum(cluster_radius))\n",
        "#       fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "#       sortIndex1 = np.argsort(d1)\n",
        "#       fet1 = tempcluster[0][sortIndex1[:int(round(fetchSize * 0.5))]]\n",
        "#       fet1 = fet1.astype(int)\n",
        "#       fil_index = sortIndex1[-int(round(len(d1) / 2)):]\n",
        "#       d2 = []\n",
        "#       for k in range(len(fil_index)):\n",
        "#           temp_d1 = np.linalg.norm(AccSample[tempcluster[0][fil_index[k]], :] - temp_neigh1)\n",
        "#           temp_d2 = np.linalg.norm(AccSample[tempcluster[0][fil_index[k]], :] - temp_neigh2)\n",
        "#           temp_ratio1 = max(temp_d1, temp_d2) / (min(temp_d1, temp_d2))\n",
        "#           temp_ratio2 = (temp_d1 + temp_d2) / (np.linalg.norm(temp_neigh1 - temp_neigh2))\n",
        "#           d2.append(temp_ratio1)\n",
        "#       sortIndex2 = np.argsort(d2)\n",
        "#       candidate_fet2 = fil_index[sortIndex2[:int(round(fetchSize * 0.8))]]\n",
        "#       sum_dist = []\n",
        "#       for ii in range(len(candidate_fet2)):\n",
        "#         candidate_d1 = np.linalg.norm(AccSample[tempcluster[0][candidate_fet2[ii]], :] - temp_neigh1)\n",
        "#         candidate_d2 = np.linalg.norm(AccSample[tempcluster[0][candidate_fet2[ii]], :] - temp_neigh1)\n",
        "#         sum_dist.append(candidate_d1 + candidate_d2)\n",
        "\n",
        "#       sortIndex3 = np.argsort(sum_dist)\n",
        "#       fet2 = tempcluster[0][candidate_fet2[sortIndex3[:int(round(fetchSize * 0.5))]]]\n",
        "#       fet2 = fet2.astype(int)\n",
        "#       FetchIndex = np.append(FetchIndex, fet1)\n",
        "#       FetchIndex = np.append(FetchIndex, fet2)\n",
        "#       UnlabeledIndex = np.append(UnlabeledIndex, tempcluster[0][sortIndex1[int(round(fetchSize * 0.5)):round(len(d1) / 2)]])\n",
        "#       UnlabeledIndex = np.append(UnlabeledIndex, tempcluster[0][fil_index[sortIndex2[int(round(fetchSize * 0.5)):len(d2)]]])\n",
        "# FetchIndex = FetchIndex.astype(int)\n",
        "# UnlabeledIndex = UnlabeledIndex.astype(int)"
      ],
      "metadata": {
        "id": "Ihrow_Bv7WlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queried_data = AccSample[FetchIndex][:]\n",
        "# queried_label1 = label[FetchIndex]\n",
        "# print(\"Number of clusters: \",np.shape(P))\n",
        "# print(\"Number of queried labels: \",len(label[FetchIndex]))\n",
        "# # Build CNN using active learned samples\n",
        "# [nq, lq] = np.shape(queried_data)\n",
        "# nt = X_test.shape[0]\n",
        "# queried_data1 = np.reshape(queried_data, (nq,32,32,3))\n",
        "# queried_label1 = np.reshape(queried_label1,(nq, 1))\n",
        "# testing_data1 = np.reshape(X_test, (nt,32,32,3))\n",
        "# testing_label1 = np.reshape(y_test, (nt,1))\n",
        "# queried_label1 = to_categorical(queried_label1, num_classes=100)\n",
        "# testing_label1 = to_categorical(testing_label1, num_classes=100)\n",
        "y_test = to_categorical(y_test, num_classes=100)\n",
        "y_train = to_categorical(y_train, num_classes=100)"
      ],
      "metadata": {
        "id": "Z2oJUEDA7j5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular CNN for Performance Evaluation**"
      ],
      "metadata": {
        "id": "dWbdiUmnSdNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Model performance using the active learned dataset\n",
        "# model = Sequential([\n",
        "#     Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "#     Conv2D(32, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Flatten(),\n",
        "#     Dense(512, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(100, activation='softmax')\n",
        "# ])\n",
        "# print(\"labels of y_train are\", np.unique(y_train[:]))\n",
        "# print(\"labels of y_test are\", np.unique(y_test[:]))\n",
        "\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "# model.fit(queried_data1, queried_label1, epochs=60)\n",
        "# loss, acc = model.evaluate(X_test, y_test)\n",
        "# print(\"Model accuracy on test data is: {:6.3f}%\".format(100 * acc))\n",
        "## Model performance using the entire training set\n",
        "# model2 = Sequential([\n",
        "#     Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "#     Conv2D(32, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Flatten(),\n",
        "#     Dense(512, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(100, activation='softmax')\n",
        "# ])\n",
        "# print(\"labels of y_train are\", np.unique(y_train[:]))\n",
        "# print(\"labels of y_test are\", np.unique(y_test[:]))\n",
        "\n",
        "# model2.compile(optimizer='adam',\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "# model2.fit(X_train, y_train, epochs=60)\n",
        "# loss2, acc2 = model2.evaluate(X_test, y_test)\n",
        "# print(\"Model accuracy on test data without active learning  is: {:6.3f}%\".format(100 * acc2))"
      ],
      "metadata": {
        "id": "HVbwxTTs7xt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Evaluation Using the Vgg16 Architecture**"
      ],
      "metadata": {
        "id": "kiaTwRKnSsfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16        import VGG16\n",
        "from tensorflow.keras.applications.resnet50        import ResNet50\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "def create_model_vgg16():\n",
        "    tf_input = Input(shape=(32, 32, 3))\n",
        "    model = VGG16(input_tensor=tf_input, include_top=False)\n",
        "    output_pooled1 = Flatten()(model.output)\n",
        "    output_pooled2 = Dense(512, activation='relu')(output_pooled1)\n",
        "    output_pooled = Dense(100, activation='softmax')(output_pooled2)\n",
        "    return Model(model.input, output_pooled)\n",
        "# Vggmodel = create_model_vgg16()\n",
        "# # Vggmodel.summary()\n",
        "# opt = SGD(lr=0.001, momentum=0.9)\n",
        "# Vggmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history = Vggmodel.fit(queried_data1, queried_label1, epochs=10)\n",
        "# score = Vggmodel.evaluate(X_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "DHncj25TBX2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model accuracy on test data is: {:6.3f}%:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79xA2URN2js",
        "outputId": "3f0c01d4-1d1a-4c25-eb59-0cdc883a7c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy on test data is: {:6.3f}%: [2.211150646209717, 0.49779999256134033]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Vggmodel2 = create_model_vgg16()\n",
        "# Vggmodel.summary()\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "Vggmodel2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history2 = Vggmodel2.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "# print(\"Model accuracy on test data is: {:6.3f}%\".format(100 * score2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFQGMX2wcxyL",
        "outputId": "88c36604-cdca-4828-a885-85a77087cb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 54s 29ms/step - loss: 2.8620 - accuracy: 0.2823\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 1.8971 - accuracy: 0.4755\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 1.5627 - accuracy: 0.5562\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 1.3231 - accuracy: 0.6113\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 1.1190 - accuracy: 0.6666\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.9340 - accuracy: 0.7152\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.7728 - accuracy: 0.7596\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6369 - accuracy: 0.7966\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5147 - accuracy: 0.8339\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 46s 30ms/step - loss: 0.4117 - accuracy: 0.8674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score2 = Vggmodel2.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Model accuracy on test data is: {:6.3f}%:\", score2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu4Zci89SQyT",
        "outputId": "5b664857-7fb6-4448-85a2-dbe28e01f657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy on test data is: {:6.3f}%: [1.77715003490448, 0.5936999917030334]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg19        import VGG19\n",
        "def create_model_vgg19():\n",
        "    tf_input = Input(shape=(32, 32, 3))\n",
        "    model = VGG19(input_tensor=tf_input, include_top=False)\n",
        "    output_pooled1 = Flatten()(model.output)\n",
        "    output_pooled2 = Dense(512, activation='relu')(output_pooled1)\n",
        "    output_pooled = Dense(100, activation='softmax')(output_pooled2)\n",
        "    return Model(model.input, output_pooled)"
      ],
      "metadata": {
        "id": "XokPi8bYPlgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vggmodel3 = create_model_vgg19()\n",
        "# Vggmodel.summary()\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "Vggmodel3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history3 = Vggmodel3.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "score3 = Vggmodel3.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Model accuracy on test data is: {:6.3f}%:\", score3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-pYgHpFPupD",
        "outputId": "132c8e72-ec81-4118-edde-aa6de714c0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 60s 38ms/step - loss: 2.9727 - accuracy: 0.2638\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 58s 37ms/step - loss: 2.0316 - accuracy: 0.4445\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 59s 37ms/step - loss: 1.6931 - accuracy: 0.5220\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 58s 37ms/step - loss: 1.4580 - accuracy: 0.5815\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 58s 37ms/step - loss: 1.2633 - accuracy: 0.6281\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 58s 37ms/step - loss: 1.0806 - accuracy: 0.6756\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 58s 37ms/step - loss: 0.9221 - accuracy: 0.7184\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 58s 37ms/step - loss: 0.7792 - accuracy: 0.7570\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 58s 37ms/step - loss: 0.6546 - accuracy: 0.7918\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 59s 37ms/step - loss: 0.5355 - accuracy: 0.8270\n",
            "Model accuracy on test data is: {:6.3f}%: [1.84959077835083, 0.5659000277519226]\n"
          ]
        }
      ]
    }
  ]
}

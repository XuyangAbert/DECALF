{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTvYtG9wmKZZQPNbeCW53j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "392c8a14498a41f1a319dcd9d6238993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00b26deb547f44b89b2127276de23102",
              "IPY_MODEL_f4eb62b6777d4599bc4997e3ad9318a8",
              "IPY_MODEL_be22d7978e2d4e9dba9058403f2dba7a"
            ],
            "layout": "IPY_MODEL_c6f964d2d5c44aeeaba8c49659ed2330"
          }
        },
        "00b26deb547f44b89b2127276de23102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237aa623307045a480da0cb273cdce5d",
            "placeholder": "​",
            "style": "IPY_MODEL_b8d6ce0b7fdb4854aa3aac4873fefcb4",
            "value": "Optimization Progress: 100%"
          }
        },
        "f4eb62b6777d4599bc4997e3ad9318a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9b7c6042ca44172be8eea62791eca35",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_baa74235bfb941b3a9eedf012bf76e52",
            "value": 300
          }
        },
        "be22d7978e2d4e9dba9058403f2dba7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c89bd513a0d48b7a7b683084552f944",
            "placeholder": "​",
            "style": "IPY_MODEL_cbad6f8e74b844cf8e5af368663a46ad",
            "value": " 300/300 [08:09&lt;00:00,  1.45s/pipeline]"
          }
        },
        "c6f964d2d5c44aeeaba8c49659ed2330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "237aa623307045a480da0cb273cdce5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8d6ce0b7fdb4854aa3aac4873fefcb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9b7c6042ca44172be8eea62791eca35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baa74235bfb941b3a9eedf012bf76e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c89bd513a0d48b7a7b683084552f944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbad6f8e74b844cf8e5af368663a46ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuyangAbert/DeepALCS/blob/main/DeepAL3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cp-h5ErvVk0",
        "outputId": "d9359218-9985-4ca2-a492-3921ec9d9414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist,squareform\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# from featureselection import SFSFC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import f1_score,precision_score,auc, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from math import exp\n",
        "import numpy.matlib\n",
        "# import tensorflow.compat.v1 as tf\n",
        "import tensorflow as tf\n",
        "# from keras.applications.resnet50     import ResNet50\n",
        "# from keras.applications.vgg16        import VGG16\n",
        "# from keras.applications.vgg19        import VGG19\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "# from keras.optimizers import SGD, Adam\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Input, Dense, AveragePooling2D, GlobalAveragePooling2D, MaxPool2D\n",
        "from keras.applications.inception_v3 import preprocess_input as incv3_preprocess_input\n",
        "# from keras.applications.resnet50     import preprocess_input as resnet50_preprocess_input\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10, cifar100,fashion_mnist\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.applications.vgg16        import preprocess_input as vgg16_preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "# from livelossplot.inputs.keras import PlotLossesCallback"
      ],
      "metadata": {
        "id": "VWlYJpTGwJCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tpot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmEEYE4No_QF",
        "outputId": "03c48b3c-d058-464c-e237-63fc4b2bbb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tpot\n",
            "  Downloading TPOT-0.11.7-py3-none-any.whl (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.0.2)\n",
            "Collecting update-checker>=0.16\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting stopit>=1.1.1\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.7/dist-packages (from tpot) (4.64.0)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.3.5)\n",
            "Collecting deap>=1.2\n",
            "  Downloading deap-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting xgboost>=1.1.0\n",
            "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 192.9 MB 80 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from tpot) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->tpot) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from update-checker>=0.16->tpot) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)\n",
            "Building wheels for collected packages: stopit\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11956 sha256=84937b4690f669d8d5fb62612c2100e933ffe9d0591b0fce2cbe728257fefc5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/d2/79/eaf81edb391e27c87f51b8ef901ecc85a5363dc96b8b8d71e3\n",
            "Successfully built stopit\n",
            "Installing collected packages: xgboost, update-checker, stopit, deap, tpot\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed deap-1.3.1 stopit-1.1.2 tpot-0.11.7 update-checker-0.18.0 xgboost-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tpot"
      ],
      "metadata": {
        "id": "E96Ki6j0pWDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from tpot import TPOTClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# define model evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define search\n",
        "model = TPOTClassifier(generations=5, population_size=50, cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=-1)\n",
        "# perform the search\n",
        "model.fit(X, y)\n",
        "# export the best model\n",
        "model.export('tpot_best_model.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "392c8a14498a41f1a319dcd9d6238993",
            "00b26deb547f44b89b2127276de23102",
            "f4eb62b6777d4599bc4997e3ad9318a8",
            "be22d7978e2d4e9dba9058403f2dba7a",
            "c6f964d2d5c44aeeaba8c49659ed2330",
            "237aa623307045a480da0cb273cdce5d",
            "b8d6ce0b7fdb4854aa3aac4873fefcb4",
            "e9b7c6042ca44172be8eea62791eca35",
            "baa74235bfb941b3a9eedf012bf76e52",
            "4c89bd513a0d48b7a7b683084552f944",
            "cbad6f8e74b844cf8e5af368663a46ad"
          ]
        },
        "id": "TJIczDtvpa35",
        "outputId": "61dac6c1-0d0d-43ef-fcb3-b1c94255f6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "392c8a14498a41f1a319dcd9d6238993"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generation 1 - Current best internal CV score: 0.9399999999999998\n",
            "\n",
            "Generation 2 - Current best internal CV score: 0.9399999999999998\n",
            "\n",
            "Generation 3 - Current best internal CV score: 0.9399999999999998\n",
            "\n",
            "Generation 4 - Current best internal CV score: 0.9466666666666667\n",
            "\n",
            "Generation 5 - Current best internal CV score: 0.9466666666666667\n",
            "\n",
            "Best pipeline: MLPClassifier(LinearSVC(input_matrix, C=5.0, dual=True, loss=squared_hinge, penalty=l2, tol=0.0001), alpha=0.001, learning_rate_init=0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Pre_Data(data):\n",
        "    [N, L] = np.shape(data)\n",
        "    NewData = np.empty((N, L))\n",
        "    for i in range(L):\n",
        "        Temp = data[:, i]\n",
        "        NewData[:, i] = Temp\n",
        "    return NewData\n",
        "\n",
        "\n",
        "def ParamSpe(data):\n",
        "    Buffersize = 1000\n",
        "    PreStd = []\n",
        "    P_Summary = []\n",
        "    PFS = []\n",
        "    T = round(np.shape(data)[0] / Buffersize)\n",
        "    return Buffersize, P_Summary, T, PFS, PreStd\n",
        "\n",
        "\n",
        "def FeatureType(data):\n",
        "    [N, dim] = np.shape(data)\n",
        "    th = round(N ** 0.5)\n",
        "    F_cont = []\n",
        "    F_disc = []\n",
        "    F_zero = []\n",
        "    for j in range(dim):\n",
        "        temp_unique = np.unique(data[:, j])\n",
        "        if len(temp_unique) > th:\n",
        "            F_cont.append(j)\n",
        "        elif len(temp_unique) > 1:\n",
        "            F_disc.append(j)\n",
        "        if len(temp_unique) == 1:\n",
        "            F_zero.append(j)\n",
        "    return F_cont, F_disc, F_zero\n",
        "\n",
        "\n",
        "def Distance_Cal(data):\n",
        "    D = pdist(data)\n",
        "    Dist = squareform(D)\n",
        "    return Dist\n",
        "\n",
        "\n",
        "def Fitness_Cal(sample, pop, stdData, gamma):\n",
        "    Ns = np.shape(sample)[0]\n",
        "    Np = np.shape(pop)[0]\n",
        "    Newsample = np.concatenate([sample, pop])\n",
        "    Dist = Distance_Cal(Newsample)\n",
        "    fitness = []\n",
        "    for i in range(Np):\n",
        "        distArray = np.power(Dist[i + Ns, 0:Ns], 2)\n",
        "        temp = np.power(np.exp(-distArray / stdData), gamma)\n",
        "        fitness.append(np.sum(temp))\n",
        "    return fitness\n",
        "\n",
        "\n",
        "def fitness_update(P_Summary, Current, fitness, PreStd, gamma, stdData):\n",
        "    [N, dim] = np.shape(Current)\n",
        "    t_I = len(PreStd)\n",
        "    NewFit = fitness\n",
        "    if len(P_Summary) > 0:\n",
        "        PreFit = P_Summary[:, dim]\n",
        "        PreP = P_Summary[:, 0:dim]\n",
        "        OldStd = PreStd[t_I - 1]\n",
        "        for i in range(N):\n",
        "            fitin = 0\n",
        "            for j in range(np.shape(PreP)[0]):\n",
        "                if np.linalg.norm(Current[i][:] - PreP[j][:]) < 0.01:\n",
        "                    fitin = PreFit[j]\n",
        "                    break\n",
        "                else:\n",
        "                    d = np.linalg.norm(Current[i][:] - PreP[j][:])\n",
        "                    fitin += (exp(-d ** 2 / stdData) ** gamma) * (PreFit[j] ** (OldStd / stdData))\n",
        "            NewFit[i] = fitness[i] + fitin\n",
        "    return NewFit\n",
        "\n",
        "\n",
        "def PopInitial(sample, PreMu, PreStd, Buffersize):\n",
        "    [N, L] = np.shape(sample)\n",
        "    pop_Size = round(1 * N)\n",
        "    # Compute the statistics of the current data chunk\n",
        "    minLimit = np.min(sample, axis=0)\n",
        "    meanData = np.mean(sample, axis=0)\n",
        "    maxLimit = np.max(sample, axis=0)\n",
        "    # Update the statistics of the data stream\n",
        "    meanData = UpdateMean(PreMu, meanData, Buffersize)\n",
        "    PreMu.append(meanData)\n",
        "    # Compute the standard deviation of the current data chunk\n",
        "    MD = np.matlib.repmat(meanData, N, 1)\n",
        "    tempSum = np.sum(np.sum((MD - sample) ** 2, axis=1))\n",
        "    stdData = tempSum / N\n",
        "    # Update the standard deviation of the data stream\n",
        "    stdData = StdUpdate(stdData, PreStd, Buffersize)\n",
        "    # Randonmly Initialize the population indices from the data chunk\n",
        "    pop_Index = np.arange(0, N)\n",
        "    pop = sample[pop_Index, :]\n",
        "    # Calculate the initial niche radius\n",
        "    radius = numpy.linalg.norm((maxLimit - minLimit)) * 0.4  # 0.6\n",
        "\n",
        "    return [stdData, pop_Index, pop, radius, PreMu, PreStd]\n",
        "\n",
        "\n",
        "def UpdateMean(PreMy, meanData, BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreMu)\n",
        "    # Update the mean of the data stream as new data chunk arrives\n",
        "    if t_P == 0:\n",
        "        newMu = meanData\n",
        "    else:\n",
        "        oldMu = PreMu[t_P - 1][:]\n",
        "        newMu = (meanData + oldMu * t_P) / (t_P + 1)\n",
        "    return newMu\n",
        "\n",
        "\n",
        "def StdUpdate(Std, PreStd, BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreStd)\n",
        "    # Update the variance of the data stream as new data chunk arrives\n",
        "    if t_P == 0:\n",
        "        newStd = Std\n",
        "    else:\n",
        "        oldStd = PreStd[t_P - 1]\n",
        "        newStd = (Std + oldStd * t_P) / (t_P + 1)\n",
        "    return newStd\n",
        "\n",
        "\n",
        "# ------------------------Parameter Estimation----------------------------#\n",
        "def CCA(sample, stdData, Dist):\n",
        "    m = 1\n",
        "    gamma = 5\n",
        "    ep = 0.998\n",
        "    N = np.shape(sample)[0]\n",
        "    while 1:\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N - 1):\n",
        "            Diff = np.power(Dist[i, :], 2)\n",
        "            temp1 = np.power(np.exp(-Diff / stdData), gamma * m)\n",
        "            temp2 = np.power(np.exp(-Diff / stdData), gamma * (m + 1))\n",
        "            den1.append(np.sum(temp1))\n",
        "            den2.append(np.sum(temp2))\n",
        "\n",
        "        y = np.corrcoef(den1, den2)[0, 1]\n",
        "\n",
        "        if y > ep:\n",
        "            break\n",
        "        m = m + 1\n",
        "    return m * gamma\n",
        "\n",
        "\n",
        "def DCCA(sample, stdData, P_Summary, gamma, dim):\n",
        "    P_Center = P_Summary[:, 0:dim]\n",
        "    P_F = P_Summary[:, dim]\n",
        "    gam1 = gamma\n",
        "    N1 = np.shape(sample)[0]\n",
        "    N2 = np.shape(P_Center)[0]\n",
        "    ep = 0.998\n",
        "    N = N1 + N2\n",
        "    temp = np.concatenate([sample, P_Center], axis=0)\n",
        "    Dist = Distance_Cal(temp)\n",
        "    while 1:\n",
        "        gam2 = gam1 + 5\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N):\n",
        "            Diff = np.power(Dist[i, 0:N1], 2)\n",
        "            temp1 = np.power(np.exp(-Diff / stdData), gam1)\n",
        "            temp2 = np.power(np.exp(-Diff / stdData), gam2)\n",
        "            sum1 = np.sum(temp1)\n",
        "            sum2 = np.sum(temp2)\n",
        "            if i < N1:\n",
        "                T1 = 0\n",
        "                T2 = 0\n",
        "                for j in range(N2):\n",
        "                    T1 += P_F[j] ** (gam1 / gamma)\n",
        "                    T2 += P_F[j] ** (gam2 / gamma)\n",
        "                s1 = sum1 + T1\n",
        "                s2 = sum2 + T2\n",
        "            #                s1 = sum1**(gam1/gamma) + T1\n",
        "            #                s2 = sum2**(gam2/gamma) + T2\n",
        "            else:\n",
        "                #                s1 = sum1**(gam1/gamma) + P_F[i-N1]**(gam1/gamma)\n",
        "                #                s2 = sum2**(gam2/gamma) + P_F[i-N1]**(gam2/gamma)\n",
        "                s1 = sum1 + P_F[i - N1] ** (gam1 / gamma)\n",
        "                s2 = sum2 + P_F[i - N1] ** (gam2 / gamma)\n",
        "            den1.append(s1)\n",
        "            den2.append(s2)\n",
        "        y = np.corrcoef(den1, den2)[0, 1]\n",
        "        if y > ep:\n",
        "            break\n",
        "        gam1 = gam2\n",
        "    return gam1\n",
        "\n",
        "\n",
        "def TPC_Search(Dist, Pop_Index, Pop, radius, fitness):\n",
        "    # Extract the size of the population\n",
        "    [N, dim] = np.shape(Pop)\n",
        "    P = []  # Initialize the Peak Vector\n",
        "    P_fitness = []\n",
        "    # i = 1\n",
        "    marked = []\n",
        "    co = []\n",
        "    OriginalIndice = Pop_Index\n",
        "    while 1:\n",
        "        # -------------Search for the local maximum-----------------#\n",
        "        SortIndice = np.argsort(fitness)\n",
        "        NewIndice = SortIndice[::-1]\n",
        "\n",
        "        Pop = Pop[NewIndice, :]\n",
        "        fitness = fitness[NewIndice]\n",
        "        OriginalIndice = OriginalIndice[NewIndice]\n",
        "\n",
        "        P.append(Pop[0, :])\n",
        "\n",
        "        P_fitness.append(fitness[0])\n",
        "        P_Indice = OriginalIndice[0]\n",
        "\n",
        "        Ind = AssigntoPeaks(Pop, Pop_Index, P, P_Indice, marked, radius, Dist)\n",
        "\n",
        "        marked.append(Ind)\n",
        "        marked.append(NewIndice[0])\n",
        "\n",
        "        if not Ind:\n",
        "            Ind = [NewIndice[0]]\n",
        "\n",
        "        co.append(len(Ind))\n",
        "        TempFit = fitness\n",
        "        sum1 = 0\n",
        "        for j in range(len(Ind)):\n",
        "            sum1 += fitness[np.where(OriginalIndice == Ind[j])]\n",
        "        for th in range(len(Ind)):\n",
        "            TempFit[np.where(OriginalIndice == Ind[th])] = fitness[np.where(OriginalIndice == Ind[th])] / sum1\n",
        "        fitness = TempFit\n",
        "        if np.sum(co) >= N:\n",
        "            P = np.asarray(P)\n",
        "            P_fitness = np.asarray(P_fitness)\n",
        "            break\n",
        "\n",
        "    return P, P_fitness\n",
        "\n",
        "\n",
        "def MergeInChunk(P, P_fitness, sample, gamma, stdData):\n",
        "    \"\"\"Perform the Merge of TPCs witnin each data chunk\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc, dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked = []\n",
        "    unmarked = []\n",
        "    Com = []\n",
        "\n",
        "    # Num of TPCs\n",
        "    Nc = np.shape(P)[0]\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j != i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j, :] - P[i, :])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice <= Nc:\n",
        "                MinIndice = int(MinIndice)\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i, :]) / 2\n",
        "\n",
        "                X = np.reshape(X, (1, np.shape(P)[1]))\n",
        "\n",
        "                fitX = Fitness_Cal(sample, X, stdData, gamma)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 0.85 * min(fitN, fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i, MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k, 0]] >= P_fitness[Com[k, 1]]:\n",
        "            NewP.append(P[Com[k, 0], :])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k, 1], :])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n, :])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP, NewP_fitness\n",
        "\n",
        "\n",
        "def MergeOnline(P, P_fitness, P_summary, PreStd, sample, gamma, stdData):\n",
        "    \"\"\"Perform the Merge of Clusters Between Historical and New Clusters\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc, dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked = []\n",
        "    unmarked = []\n",
        "    Com = []\n",
        "\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j != i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j, :] - P[i, :])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice < Nc:\n",
        "\n",
        "                #                MinIndice = int(MinIndice)\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i][:]) / 2\n",
        "                X = np.reshape(X, (1, np.shape(P)[1]))\n",
        "                RfitX = Fitness_Cal(sample, X, stdData, gamma)\n",
        "                fitX = fitness_update(P_Summary, X, RfitX, PreStd, gamma, stdData)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 0.85 * min(fitN, fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i, MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k, 0]] >= P_fitness[Com[k, 1]]:\n",
        "            NewP.append(P[Com[k, 0]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k, 1]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n][:])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP, NewP_fitness\n",
        "\n",
        "\n",
        "def CE_InChunk(sample, P, P_fitness, stdData, gamma):\n",
        "    while 1:\n",
        "        HistP = P\n",
        "        #        HistPF = P_fitness\n",
        "        P, P_fitness = MergeInChunk(P, P_fitness, sample, gamma, stdData)\n",
        "        if np.shape(P)[0] == np.shape(HistP)[0]:\n",
        "            break\n",
        "    return P, P_fitness\n",
        "\n",
        "\n",
        "def CE_Online(sample, P_Summary, P, P_fitness, stdData, gamma, PreStd):\n",
        "    dim = np.shape(P)[1]\n",
        "\n",
        "    # Concatenate the historical and new clusters together\n",
        "    PC = np.concatenate([P_Summary[:, 0:dim], P])\n",
        "    RPF = Fitness_Cal(sample, PC, stdData, gamma)\n",
        "    PF = fitness_update(P_Summary, PC, RPF, PreStd, gamma, stdData)\n",
        "\n",
        "    while 1:\n",
        "        HistPC = PC\n",
        "        #        HistPF = PF\n",
        "        PC, PF = MergeOnline(PC, PF, P_Summary, PreStd, sample, gamma, stdData)\n",
        "        RPF = Fitness_Cal(sample, PC, stdData, gamma)\n",
        "        PF = fitness_update(P_Summary, PC, RPF, PreStd, gamma, stdData)\n",
        "        if np.shape(PC)[0] == np.shape(HistPC)[0]:\n",
        "            break\n",
        "    return PC, PF\n",
        "\n",
        "\n",
        "def ClusterValidation(sample, P):\n",
        "    while 1:\n",
        "        NewP = []\n",
        "        PreP = P\n",
        "        [R_d, RIndice] = Cluster_Assign(sample, P)\n",
        "\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            Temp = np.where(RIndice == i)\n",
        "            Temp = np.asarray(Temp)\n",
        "            if np.shape(Temp)[1] > 2:\n",
        "                NewP.append(P[i][:])\n",
        "        P = NewP\n",
        "        if np.shape(P)[0] == np.shape(PreP)[0]:\n",
        "            break\n",
        "    return np.asarray(P)\n",
        "\n",
        "\n",
        "def ClusterSummary(P, PF, P_Summary, sample):\n",
        "    dim = np.shape(sample)[1]\n",
        "    Rp = AverageDist(P, P_Summary, sample, dim)\n",
        "    P = np.asarray(P)\n",
        "    PF = [PF]\n",
        "\n",
        "    PF = np.asarray(PF)\n",
        "    Rp = np.reshape(Rp, (np.shape(P)[0], 1))\n",
        "    PCluster = np.concatenate([P, PF.T], axis=1)\n",
        "    PCluster = np.concatenate([PCluster, Rp], axis=1)\n",
        "\n",
        "    P_Summary = PCluster\n",
        "\n",
        "    return P_Summary\n",
        "\n",
        "\n",
        "def StoreInf(PF, PFS, PreStd, stdData):\n",
        "    PreStd.append(stdData)\n",
        "    PFS.append(PF)\n",
        "    return PreStd, PFS\n",
        "\n",
        "\n",
        "# --------------------Cluster Radius Computation and Update--------------------#\n",
        "def AverageDist(P, P_Summary, sample, dim):\n",
        "    P = P\n",
        "    # Obtain the assignment of clusters\n",
        "    [distance, indices] = Cluster_Assign(sample, P)\n",
        "    rad1 = []\n",
        "    # if the summary of clusters is not empty\n",
        "    if len(P_Summary) > 0:\n",
        "\n",
        "        PreP = P_Summary[:, 0:dim]  # Hstorical Cluster Center vector\n",
        "        PreR = P_Summary[:, dim + 1]\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            if np.shape(np.where(indices == i))[1] > 1:\n",
        "                SumD1 = 0\n",
        "                Count1 = 0\n",
        "                for j in range(np.shape(sample)[0]):\n",
        "                    if indices[j] == i:\n",
        "                        SumD1 += distance[j]\n",
        "                        Count1 += 1\n",
        "                rad1.append(SumD1 / Count1)\n",
        "            else:\n",
        "                C_d = []\n",
        "                for k in range(np.shape(PreP)[0]):\n",
        "                    C_d.append(np.linalg.norm(P[i][:] - PreP[k][:]))\n",
        "                CI = np.argmin(C_d)\n",
        "                rad1.append(PreR[CI])\n",
        "    elif not P_Summary:\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            SumD1 = 0\n",
        "            Count1 = 0\n",
        "            for j in range(np.shape(sample)[0]):\n",
        "                if indices[j] == i:\n",
        "                    SumD1 += distance[j]\n",
        "                    Count1 += 1\n",
        "            rad1.append(SumD1 / Count1)\n",
        "    return np.asarray(rad1)\n",
        "\n",
        "\n",
        "def AssigntoPeaks(pop, pop_index, P, P_I, marked, radius, Dist):\n",
        "    temp = []\n",
        "    [N, L] = np.shape(pop)\n",
        "    for i in range(N):\n",
        "        distance = Dist[i, P_I]\n",
        "        if not np.any(marked == pop_index[i]):\n",
        "            if distance < radius:\n",
        "                temp.append(pop_index[i])\n",
        "    indices = temp\n",
        "    return indices\n",
        "\n",
        "\n",
        "def Cluster_Assign(sample, P):\n",
        "    # Number of samples\n",
        "    N = np.shape(sample)[0]\n",
        "    # Number of Clusters at t\n",
        "    Np = np.shape(P)[0]\n",
        "    MinDist = []\n",
        "    MinIndice = []\n",
        "    for i in range(N):\n",
        "        d = []\n",
        "        for j in range(Np):\n",
        "            d.append(np.linalg.norm(sample[i][:] - P[j][:]))\n",
        "        if len(d) <= 1:\n",
        "            tempD = d\n",
        "            tempI = 0\n",
        "        else:\n",
        "            tempD = np.min(d)\n",
        "            tempI = np.argmin(d)\n",
        "\n",
        "        MinDist.append(tempD)\n",
        "        MinIndice.append(tempI)\n",
        "    MinDist = np.asarray(MinDist)\n",
        "    MinIndice = np.asarray(MinIndice)\n",
        "    return MinDist, MinIndice\n",
        "def compute_radius(MinDist,ClusterIndice):\n",
        "    cluster = np.unique(ClusterIndice)\n",
        "    nc = len(cluster)\n",
        "    cluster_rad = []\n",
        "    for i in range(nc):\n",
        "        currentcluster = np.where(ClusterIndice==cluster[i])[0]\n",
        "        cluster_rad.append(np.mean(MinDist[currentcluster]))\n",
        "    return cluster_rad"
      ],
      "metadata": {
        "id": "d6Z1Xg9F7BQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering Procedure using the FPS-clustering**"
      ],
      "metadata": {
        "id": "AzjH0QWLW3JO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------Main Function-------------------------#\n",
        "if __name__ == '__main__':\n",
        "    start = time.time()\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    # (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    data = X_train.reshape(X_train.shape[0], 3072)\n",
        "    label = y_train\n",
        "\n",
        "    dim = np.shape(data)[1]\n",
        "    [BufferSize,P_Summary,T,PFS,PreStd] = ParamSpe(data)\n",
        "    T = int(T)\n",
        "    gammaHist = []\n",
        "    PFS = []\n",
        "    PreMu = []\n",
        "\n",
        "    for t in range(T):\n",
        "        if t < T-1:\n",
        "           sample = data[t*BufferSize:(t+1)*BufferSize,:]\n",
        "        else:\n",
        "            sample = data[t*BufferSize:np.shape(data)[0]]\n",
        "        if t==0:\n",
        "            AccSample = sample\n",
        "        else:\n",
        "            AccSample = np.concatenate([AccSample,sample])\n",
        "\n",
        "        [stdData,pop_index,pop,radius,PreMu,PreStd] = PopInitial(sample,PreMu,PreStd,BufferSize)\n",
        "        # Initialize the fitness vector\n",
        "        fitness = np.zeros((len(pop_index),1))\n",
        "        # Initialize the indices vector\n",
        "        indices = np.zeros((len(pop_index),1))\n",
        "        Dist = Distance_Cal(sample)\n",
        "        if PreStd:\n",
        "            if PreStd[len(PreStd)-1] > stdData:\n",
        "                P = P_Summary[:,0:dim]\n",
        "                localFit = Fitness_Cal(sample,P,stdData,gamma)\n",
        "                PF = fitness_update(P_Summary,P,localFit,PreStd,gamma,stdData)\n",
        "                P_Summary = ClusterSummary(P,PF,P_Summary,sample)\n",
        "                PFS.append(PF)\n",
        "                PreStd.append(stdData)\n",
        "                clustercenter = P\n",
        "                [Assign,clusterindex] = Cluster_Assign(AccSample,P)\n",
        "                continue\n",
        "        else:\n",
        "            gamma = CCA(sample,stdData,Dist)\n",
        "        gammaHist.append(gamma)\n",
        "        fitness = Fitness_Cal(sample,pop,stdData,gamma)\n",
        "        fitness = np.array(fitness)\n",
        "        P, P_fitness = TPC_Search(Dist,pop_index,pop,radius,fitness)\n",
        "        P, P_fitness = CE_InChunk(sample,P,P_fitness,stdData,gamma)\n",
        "        P_fitness = Fitness_Cal(sample,P,stdData,gamma)\n",
        "        P_fitness = fitness_update(P_Summary,P,P_fitness,PreStd,gamma,stdData)\n",
        "        print('Processing Data Chunk '+str(t))\n",
        "        if t == 0:\n",
        "            P = P\n",
        "            PF = np.asarray(P_fitness)\n",
        "        else:\n",
        "            P,P_fitness = CE_Online(sample,P_Summary,P,P_fitness,stdData,gamma,PreStd)\n",
        "            PF = np.asarray(P_fitness)\n",
        "        P_Summary = ClusterSummary(P,PF,P_Summary,sample)\n",
        "        PreStd,PFS = StoreInf(PF,PFS,PreStd,stdData)\n",
        "    # Clustering procedure finishes\n",
        "    [MinDist,ClusterIndice] = Cluster_Assign(AccSample,P)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcR9qI_I7OWA",
        "outputId": "0d93fda6-5a76-4f38-e07b-143c5a45adae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:473: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Data Chunk 0\n",
            "Processing Data Chunk 2\n",
            "Processing Data Chunk 3\n",
            "Processing Data Chunk 4\n",
            "Processing Data Chunk 5\n",
            "Processing Data Chunk 7\n",
            "Processing Data Chunk 8\n",
            "Processing Data Chunk 13\n",
            "Processing Data Chunk 15\n",
            "Processing Data Chunk 16\n",
            "Processing Data Chunk 18\n",
            "Processing Data Chunk 19\n",
            "Processing Data Chunk 20\n",
            "Processing Data Chunk 21\n",
            "Processing Data Chunk 23\n",
            "Processing Data Chunk 27\n",
            "Processing Data Chunk 29\n",
            "Processing Data Chunk 30\n",
            "Processing Data Chunk 31\n",
            "Processing Data Chunk 40\n",
            "Processing Data Chunk 43\n",
            "Processing Data Chunk 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Active Learning Phase**"
      ],
      "metadata": {
        "id": "LPxKuFFXWvOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Active Learning\n",
        "sample_size = np.shape(AccSample)[0]\n",
        "num_S = round(0.4 * sample_size)\n",
        "cluster_radius = compute_radius(MinDist, ClusterIndice)\n",
        "delta = np.mean(cluster_radius)\n",
        "sigma = np.std(cluster_radius)\n",
        "dense_idx = np.where(cluster_radius <= abs(delta - sigma))[0]\n",
        "sparese_idx = np.where(cluster_radius > abs(delta - sigma))[0]\n",
        "\n",
        "FetchIndex = []\n",
        "UnlabeledIndex = []\n",
        "InterDist = squareform(pdist(P))\n",
        "\n",
        "FetchIndex = []\n",
        "UnlabeledIndex = []\n",
        "InterDist = squareform(pdist(P))\n",
        "\n",
        "for i in range(np.shape(P)[0]):\n",
        "  tempcluster = np.where(ClusterIndice == (i))\n",
        "  d1 = []\n",
        "  if i in dense_idx:\n",
        "    for j in range(len(tempcluster[0])):\n",
        "      d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "    fetchSize = round(num_S * cluster_radius[i] / (np.sum(cluster_radius)))\n",
        "    fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "    sortIndex1 = np.argsort(d1)\n",
        "    fet1 = tempcluster[0][sortIndex1[:int(round(fetchSize * 1))]]\n",
        "    fet1 = fet1.astype(int)\n",
        "    fet2 = []\n",
        "    FetchIndex = np.append(FetchIndex, fet1)\n",
        "    FetchIndex = np.append(FetchIndex, fet2)\n",
        "    UnlabeledIndex = np.append(UnlabeledIndex,tempcluster[0][sortIndex1[int(round(fetchSize * 1)):round(len(d1))]])\n",
        "  else:\n",
        "      temp_interdist = InterDist[i, :]\n",
        "      temp_rank = np.argsort(temp_interdist)\n",
        "      temp_neigh1 = P[temp_rank[0], :]\n",
        "      temp_neigh2 = P[temp_rank[1], :]\n",
        "      d1 = []\n",
        "      for j in range(len(tempcluster[0])):\n",
        "        d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "      # fetchSize = round(num_S * cluster_radius[i] / np.sum(cluster_radius))\n",
        "      fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "      sortIndex1 = np.argsort(d1)\n",
        "      fet1 = tempcluster[0][sortIndex1[:int(round(fetchSize * 0.5))]]\n",
        "      fet1 = fet1.astype(int)\n",
        "      fil_index = sortIndex1[-int(round(len(d1) / 2)):]\n",
        "      d2 = []\n",
        "      for k in range(len(fil_index)):\n",
        "          temp_d1 = np.linalg.norm(AccSample[tempcluster[0][fil_index[k]], :] - temp_neigh1)\n",
        "          temp_d2 = np.linalg.norm(AccSample[tempcluster[0][fil_index[k]], :] - temp_neigh2)\n",
        "          temp_ratio1 = max(temp_d1, temp_d2) / (min(temp_d1, temp_d2))\n",
        "          temp_ratio2 = (temp_d1 + temp_d2) / (np.linalg.norm(temp_neigh1 - temp_neigh2))\n",
        "          d2.append(temp_ratio1)\n",
        "      sortIndex2 = np.argsort(d2)\n",
        "      candidate_fet2 = fil_index[sortIndex2[:int(round(fetchSize * 0.8))]]\n",
        "      sum_dist = []\n",
        "      for ii in range(len(candidate_fet2)):\n",
        "        candidate_d1 = np.linalg.norm(AccSample[tempcluster[0][candidate_fet2[ii]], :] - temp_neigh1)\n",
        "        candidate_d2 = np.linalg.norm(AccSample[tempcluster[0][candidate_fet2[ii]], :] - temp_neigh1)\n",
        "        sum_dist.append(candidate_d1 + candidate_d2)\n",
        "\n",
        "      sortIndex3 = np.argsort(sum_dist)\n",
        "      fet2 = tempcluster[0][candidate_fet2[sortIndex3[:int(round(fetchSize * 0.5))]]]\n",
        "      fet2 = fet2.astype(int)\n",
        "      FetchIndex = np.append(FetchIndex, fet1)\n",
        "      FetchIndex = np.append(FetchIndex, fet2)\n",
        "      UnlabeledIndex = np.append(UnlabeledIndex, tempcluster[0][sortIndex1[int(round(fetchSize * 0.5)):round(len(d1) / 2)]])\n",
        "      UnlabeledIndex = np.append(UnlabeledIndex, tempcluster[0][fil_index[sortIndex2[int(round(fetchSize * 0.5)):len(d2)]]])\n",
        "FetchIndex = FetchIndex.astype(int)\n",
        "UnlabeledIndex = UnlabeledIndex.astype(int)"
      ],
      "metadata": {
        "id": "Ihrow_Bv7WlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queried_data = AccSample[FetchIndex][:]\n",
        "queried_label1 = label[FetchIndex]\n",
        "print(\"Number of clusters: \",np.shape(P))\n",
        "print(\"Number of queried labels: \",len(label[FetchIndex]))\n",
        "# Build CNN using active learned samples\n",
        "[nq, lq] = np.shape(queried_data)\n",
        "nt = X_test.shape[0]\n",
        "queried_data1 = np.reshape(queried_data, (nq,32,32,3))\n",
        "queried_label1 = np.reshape(queried_label1,(nq, 1))\n",
        "testing_data1 = np.reshape(X_test, (nt,32,32,3))\n",
        "testing_label1 = np.reshape(y_test, (nt,1))\n",
        "queried_label1 = to_categorical(queried_label1, num_classes=10)\n",
        "testing_label1 = to_categorical(testing_label1, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "y_train = to_categorical(y_train, num_classes=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2oJUEDA7j5i",
        "outputId": "6b1bbd61-2bdd-443a-ed2d-44d503173016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters:  (40, 3072)\n",
            "Number of queried labels:  20005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Evaluation using the regular CNN model**"
      ],
      "metadata": {
        "id": "ESu3UhMeWTKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential([\n",
        "#     Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "#     Conv2D(32, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Flatten(),\n",
        "#     Dense(512, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(10, activation='softmax')\n",
        "# ])\n",
        "# print(\"labels of y_train are\", np.unique(y_train[:]))\n",
        "# print(\"labels of y_test are\", np.unique(y_test[:]))\n",
        "\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "# model.fit(queried_data1, queried_label1, epochs=20)\n",
        "# loss, acc = model.evaluate(X_test, y_test)\n",
        "# print(\"Model accuracy on test data is: {:6.3f}%\".format(100 * acc))\n",
        "############Model performance using the entire training set ##########\n",
        "# model2 = Sequential([\n",
        "#     Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "#     Conv2D(32, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     Conv2D(64, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2, 2),\n",
        "#     Dropout(0.3),\n",
        "#     Flatten(),\n",
        "#     Dense(512, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(10, activation='softmax')\n",
        "# ])\n",
        "# print(\"labels of y_train are\", np.unique(y_train[:]))\n",
        "# print(\"labels of y_test are\", np.unique(y_test[:]))\n",
        "\n",
        "# model2.compile(optimizer='adam',\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "# model2.fit(X_train, y_train, epochs=20)\n",
        "# loss2, acc2 = model2.evaluate(X_test, y_test)\n",
        "# print(\"Model accuracy on test data without active learning  is: {:6.3f}%\".format(100 * acc2))"
      ],
      "metadata": {
        "id": "HVbwxTTs7xt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Evaluation using Vgg16 Architecture**"
      ],
      "metadata": {
        "id": "mQ0L1vgyVgtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16        import VGG16\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "def create_model_vgg16():\n",
        "    tf_input = Input(shape=(32, 32, 3))\n",
        "    model = VGG16(input_tensor=tf_input, include_top=False)\n",
        "    output_pooled1 = Flatten()(model.output)\n",
        "    output_pooled2 = Dense(512, activation='relu')(output_pooled1)\n",
        "    output_pooled = Dense(10, activation='softmax')(output_pooled2)\n",
        "    return Model(model.input, output_pooled)\n",
        "Vggmodel = create_model_vgg16()\n",
        "# Vggmodel.summary()\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "Vggmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = Vggmodel.fit(queried_data1, queried_label1, epochs=10)\n",
        "loss, score = Vggmodel.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Model accuracy on test data is: {:6.3f}%\".format(100 * score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRRMARy1VXZX",
        "outputId": "c6367c06-1d34-4fa8-8b3d-aba33a2186a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "626/626 [==============================] - 48s 61ms/step - loss: 1.1040 - accuracy: 0.6119\n",
            "Epoch 2/10\n",
            "626/626 [==============================] - 37s 60ms/step - loss: 0.6695 - accuracy: 0.7736\n",
            "Epoch 3/10\n",
            "626/626 [==============================] - 37s 60ms/step - loss: 0.5164 - accuracy: 0.8238\n",
            "Epoch 4/10\n",
            "626/626 [==============================] - 37s 59ms/step - loss: 0.4031 - accuracy: 0.8616\n",
            "Epoch 5/10\n",
            "626/626 [==============================] - 37s 60ms/step - loss: 0.3196 - accuracy: 0.8897\n",
            "Epoch 6/10\n",
            "626/626 [==============================] - 37s 59ms/step - loss: 0.2466 - accuracy: 0.9158\n",
            "Epoch 7/10\n",
            "626/626 [==============================] - 37s 59ms/step - loss: 0.1743 - accuracy: 0.9398\n",
            "Epoch 8/10\n",
            "626/626 [==============================] - 38s 60ms/step - loss: 0.1260 - accuracy: 0.9574\n",
            "Epoch 9/10\n",
            "626/626 [==============================] - 38s 61ms/step - loss: 0.1051 - accuracy: 0.9640\n",
            "Epoch 10/10\n",
            "626/626 [==============================] - 38s 60ms/step - loss: 0.0711 - accuracy: 0.9760\n",
            "Model accuracy on test data is: 81.550%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VGG16 Performance using the entire training set**"
      ],
      "metadata": {
        "id": "zvasMXW8eZqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Vggmodel2 = create_model_vgg16()\n",
        "# Vggmodel.summary()\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "Vggmodel2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history2 = Vggmodel2.fit(X_train, y_train, epochs=10)\n",
        "loss2,score2 = Vggmodel2.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Model accuracy on test data is: {:6.3f}%\".format(100 * score2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFQGMX2wcxyL",
        "outputId": "27e3ac44-6468-4fb4-84b4-d075492a767b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 96s 61ms/step - loss: 0.8652 - accuracy: 0.6970\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.5141 - accuracy: 0.8214\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.3888 - accuracy: 0.8649\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 0.3025 - accuracy: 0.8931\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 93s 60ms/step - loss: 0.2259 - accuracy: 0.9222\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.1674 - accuracy: 0.9428\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.1252 - accuracy: 0.9570\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.0926 - accuracy: 0.9688\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.0737 - accuracy: 0.9744\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 94s 60ms/step - loss: 0.0535 - accuracy: 0.9812\n",
            "Model accuracy on test data is: 84.480%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "-UlF9nNilBuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
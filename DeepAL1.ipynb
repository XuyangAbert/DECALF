{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuyangAbert/DeepALCS/blob/main/DeepAL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6jNCdFrYNLo",
        "outputId": "f51a2220-090b-4705-c75f-80250dfcf288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pul4nPJYjmY",
        "outputId": "9e172914-95bc-4ee6-b9ed-4b129ca7b934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/svhn\n"
          ]
        }
      ],
      "source": [
        "cd /content/gdrive/MyDrive/svhn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeUl1ZU0Y2L2",
        "outputId": "c182543d-7169-414a-afd7-4ed9d6718989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-27 01:32:35--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘train_32x32.mat’\n",
            "\n",
            "train_32x32.mat     100%[===================>] 173.61M  30.6MB/s    in 8.4s    \n",
            "\n",
            "2022-04-27 01:32:43 (20.7 MB/s) - ‘train_32x32.mat’ saved [182040794/182040794]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \"http://ufldl.stanford.edu/housenumbers/train_32x32.mat\" -O \"train_32x32.mat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4hKW0a9Y8v8",
        "outputId": "50fec023-006a-43a8-ccc0-8eec307ef891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-27 01:32:45--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64275384 (61M) [text/plain]\n",
            "Saving to: ‘test_32x32.mat’\n",
            "\n",
            "test_32x32.mat      100%[===================>]  61.30M  17.1MB/s    in 4.2s    \n",
            "\n",
            "2022-04-27 01:32:49 (14.6 MB/s) - ‘test_32x32.mat’ saved [64275384/64275384]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \"http://ufldl.stanford.edu/housenumbers/test_32x32.mat\" -O \"test_32x32.mat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB6SD-d7Y_PD",
        "outputId": "bbffcc93-9534-4c53-9265-19db2d3ef646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(73257, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "\n",
        "train = loadmat('train_32x32.mat')\n",
        "test = loadmat('test_32x32.mat')\n",
        "\n",
        "# train and test are python dictionaries\n",
        "# keys are ['__header__', '__version__', '__globals__', 'X', 'y']\n",
        "\n",
        "X_train = train['X']\n",
        "y_train = train['y']\n",
        "X_test = test['X']\n",
        "y_test = test['y']\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = np.rollaxis(X_train, 3)\n",
        "X_test = np.rollaxis(X_test, 3)\n",
        "y_train[y_train==10] = 0\n",
        "y_test[y_test==10] = 0\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE5kQWzXZItZ"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist,squareform\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# from featureselection import SFSFC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import f1_score,precision_score,auc, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from math import exp\n",
        "import numpy.matlib\n",
        "# import tensorflow.compat.v1 as tf\n",
        "import tensorflow as tf\n",
        "# from keras.applications.resnet50     import ResNet50\n",
        "# from keras.applications.vgg16        import VGG16\n",
        "# from keras.applications.vgg19        import VGG19\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "# from keras.optimizers import SGD, Adam\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Input, Dense, AveragePooling2D, GlobalAveragePooling2D, MaxPool2D\n",
        "from keras.applications.inception_v3 import preprocess_input as incv3_preprocess_input\n",
        "# from keras.applications.resnet50     import preprocess_input as resnet50_preprocess_input\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10, cifar100,fashion_mnist\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.applications.vgg16        import preprocess_input as vgg16_preprocess_input\n",
        "# from livelossplot.inputs.keras import PlotLossesCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-bUlcY3ZX2R"
      },
      "outputs": [],
      "source": [
        "def Pre_Data(data):\n",
        "    [N, L] = np.shape(data)\n",
        "    NewData = np.empty((N, L))\n",
        "    for i in range(L):\n",
        "        Temp = data[:, i]\n",
        "        NewData[:, i] = Temp\n",
        "    return NewData\n",
        "\n",
        "\n",
        "def ParamSpe(data):\n",
        "    Buffersize = 1000\n",
        "    PreStd = []\n",
        "    P_Summary = []\n",
        "    PFS = []\n",
        "    T = round(np.shape(data)[0] / Buffersize)\n",
        "    return Buffersize, P_Summary, T, PFS, PreStd\n",
        "\n",
        "\n",
        "def FeatureType(data):\n",
        "    [N, dim] = np.shape(data)\n",
        "    th = round(N ** 0.5)\n",
        "    F_cont = []\n",
        "    F_disc = []\n",
        "    F_zero = []\n",
        "    for j in range(dim):\n",
        "        temp_unique = np.unique(data[:, j])\n",
        "        if len(temp_unique) > th:\n",
        "            F_cont.append(j)\n",
        "        elif len(temp_unique) > 1:\n",
        "            F_disc.append(j)\n",
        "        if len(temp_unique) == 1:\n",
        "            F_zero.append(j)\n",
        "    return F_cont, F_disc, F_zero\n",
        "\n",
        "\n",
        "def Distance_Cal(data):\n",
        "    D = pdist(data)\n",
        "    Dist = squareform(D)\n",
        "    return Dist\n",
        "\n",
        "\n",
        "def Fitness_Cal(sample, pop, stdData, gamma):\n",
        "    Ns = np.shape(sample)[0]\n",
        "    Np = np.shape(pop)[0]\n",
        "    Newsample = np.concatenate([sample, pop])\n",
        "    Dist = Distance_Cal(Newsample)\n",
        "    fitness = []\n",
        "    for i in range(Np):\n",
        "        distArray = np.power(Dist[i + Ns, 0:Ns], 2)\n",
        "        temp = np.power(np.exp(-distArray / stdData), gamma)\n",
        "        fitness.append(np.sum(temp))\n",
        "    return fitness\n",
        "\n",
        "\n",
        "def fitness_update(P_Summary, Current, fitness, PreStd, gamma, stdData):\n",
        "    [N, dim] = np.shape(Current)\n",
        "    t_I = len(PreStd)\n",
        "    NewFit = fitness\n",
        "    if len(P_Summary) > 0:\n",
        "        PreFit = P_Summary[:, dim]\n",
        "        PreP = P_Summary[:, 0:dim]\n",
        "        OldStd = PreStd[t_I - 1]\n",
        "        for i in range(N):\n",
        "            fitin = 0\n",
        "            for j in range(np.shape(PreP)[0]):\n",
        "                if np.linalg.norm(Current[i][:] - PreP[j][:]) < 0.01:\n",
        "                    fitin = PreFit[j]\n",
        "                    break\n",
        "                else:\n",
        "                    d = np.linalg.norm(Current[i][:] - PreP[j][:])\n",
        "                    fitin += (exp(-d ** 2 / stdData) ** gamma) * (PreFit[j] ** (OldStd / stdData))\n",
        "            NewFit[i] = fitness[i] + fitin\n",
        "    return NewFit\n",
        "\n",
        "\n",
        "def PopInitial(sample, PreMu, PreStd, Buffersize):\n",
        "    [N, L] = np.shape(sample)\n",
        "    pop_Size = round(1 * N)\n",
        "    # Compute the statistics of the current data chunk\n",
        "    minLimit = np.min(sample, axis=0)\n",
        "    meanData = np.mean(sample, axis=0)\n",
        "    maxLimit = np.max(sample, axis=0)\n",
        "    # Update the statistics of the data stream\n",
        "    meanData = UpdateMean(PreMu, meanData, Buffersize)\n",
        "    PreMu.append(meanData)\n",
        "    # Compute the standard deviation of the current data chunk\n",
        "    MD = np.matlib.repmat(meanData, N, 1)\n",
        "    tempSum = np.sum(np.sum((MD - sample) ** 2, axis=1))\n",
        "    stdData = tempSum / N\n",
        "    # Update the standard deviation of the data stream\n",
        "    stdData = StdUpdate(stdData, PreStd, Buffersize)\n",
        "    # Randonmly Initialize the population indices from the data chunk\n",
        "    pop_Index = np.arange(0, N)\n",
        "    pop = sample[pop_Index, :]\n",
        "    # Calculate the initial niche radius\n",
        "    radius = numpy.linalg.norm((maxLimit - minLimit)) * 0.4  # 0.3\n",
        "    return [stdData, pop_Index, pop, radius, PreMu, PreStd]\n",
        "\n",
        "\n",
        "def UpdateMean(PreMy, meanData, BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreMu)\n",
        "    # Update the mean of the data stream as new data chunk arrives\n",
        "    if t_P == 0:\n",
        "        newMu = meanData\n",
        "    else:\n",
        "        oldMu = PreMu[t_P - 1][:]\n",
        "        newMu = (meanData + oldMu * t_P) / (t_P + 1)\n",
        "    return newMu\n",
        "\n",
        "\n",
        "def StdUpdate(Std, PreStd, BufferSize):\n",
        "    # Num of the processed data chunk\n",
        "    t_P = len(PreStd)\n",
        "    # Update the variance of the data stream as new data chunk arrives\n",
        "    if t_P == 0:\n",
        "        newStd = Std\n",
        "    else:\n",
        "        oldStd = PreStd[t_P - 1]\n",
        "        newStd = (Std + oldStd * t_P) / (t_P + 1)\n",
        "    return newStd\n",
        "\n",
        "\n",
        "# ------------------------Parameter Estimation----------------------------#\n",
        "def CCA(sample, stdData, Dist):\n",
        "    m = 1\n",
        "    gamma = 5\n",
        "    ep = 0.995\n",
        "    N = np.shape(sample)[0]\n",
        "    while 1:\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N - 1):\n",
        "            Diff = np.power(Dist[i, :], 2)\n",
        "            temp1 = np.power(np.exp(-Diff / stdData), gamma * m)\n",
        "            temp2 = np.power(np.exp(-Diff / stdData), gamma * (m + 1))\n",
        "            den1.append(np.sum(temp1))\n",
        "            den2.append(np.sum(temp2))\n",
        "\n",
        "        y = np.corrcoef(den1, den2)[0, 1]\n",
        "\n",
        "        if y > ep:\n",
        "            break\n",
        "        m = m + 1\n",
        "    return m * gamma\n",
        "\n",
        "\n",
        "def DCCA(sample, stdData, P_Summary, gamma, dim):\n",
        "    P_Center = P_Summary[:, 0:dim]\n",
        "    P_F = P_Summary[:, dim]\n",
        "    gam1 = gamma\n",
        "    N1 = np.shape(sample)[0]\n",
        "    N2 = np.shape(P_Center)[0]\n",
        "    ep = 0.995\n",
        "    N = N1 + N2\n",
        "    temp = np.concatenate([sample, P_Center], axis=0)\n",
        "    Dist = Distance_Cal(temp)\n",
        "    while 1:\n",
        "        gam2 = gam1 + 5\n",
        "        den1 = []\n",
        "        den2 = []\n",
        "        for i in range(N):\n",
        "            Diff = np.power(Dist[i, 0:N1], 2)\n",
        "            temp1 = np.power(np.exp(-Diff / stdData), gam1)\n",
        "            temp2 = np.power(np.exp(-Diff / stdData), gam2)\n",
        "            sum1 = np.sum(temp1)\n",
        "            sum2 = np.sum(temp2)\n",
        "            if i < N1:\n",
        "                T1 = 0\n",
        "                T2 = 0\n",
        "                for j in range(N2):\n",
        "                    T1 += P_F[j] ** (gam1 / gamma)\n",
        "                    T2 += P_F[j] ** (gam2 / gamma)\n",
        "                s1 = sum1 + T1\n",
        "                s2 = sum2 + T2\n",
        "            #                s1 = sum1**(gam1/gamma) + T1\n",
        "            #                s2 = sum2**(gam2/gamma) + T2\n",
        "            else:\n",
        "                #                s1 = sum1**(gam1/gamma) + P_F[i-N1]**(gam1/gamma)\n",
        "                #                s2 = sum2**(gam2/gamma) + P_F[i-N1]**(gam2/gamma)\n",
        "                s1 = sum1 + P_F[i - N1] ** (gam1 / gamma)\n",
        "                s2 = sum2 + P_F[i - N1] ** (gam2 / gamma)\n",
        "            den1.append(s1)\n",
        "            den2.append(s2)\n",
        "        y = np.corrcoef(den1, den2)[0, 1]\n",
        "        if y > ep:\n",
        "            break\n",
        "        gam1 = gam2\n",
        "    return gam1\n",
        "\n",
        "\n",
        "def TPC_Search(Dist, Pop_Index, Pop, radius, fitness):\n",
        "    # Extract the size of the population\n",
        "    [N, dim] = np.shape(Pop)\n",
        "    P = []  # Initialize the Peak Vector\n",
        "    P_fitness = []\n",
        "    # i = 1\n",
        "    marked = []\n",
        "    co = []\n",
        "    OriginalIndice = Pop_Index\n",
        "    while 1:\n",
        "        # -------------Search for the local maximum-----------------#\n",
        "        SortIndice = np.argsort(fitness)\n",
        "        NewIndice = SortIndice[::-1]\n",
        "\n",
        "        Pop = Pop[NewIndice, :]\n",
        "        fitness = fitness[NewIndice]\n",
        "        OriginalIndice = OriginalIndice[NewIndice]\n",
        "\n",
        "        P.append(Pop[0, :])\n",
        "\n",
        "        P_fitness.append(fitness[0])\n",
        "        P_Indice = OriginalIndice[0]\n",
        "\n",
        "        Ind = AssigntoPeaks(Pop, Pop_Index, P, P_Indice, marked, radius, Dist)\n",
        "\n",
        "        marked.append(Ind)\n",
        "        marked.append(NewIndice[0])\n",
        "\n",
        "        if not Ind:\n",
        "            Ind = [NewIndice[0]]\n",
        "\n",
        "        co.append(len(Ind))\n",
        "        TempFit = fitness\n",
        "        sum1 = 0\n",
        "        for j in range(len(Ind)):\n",
        "            sum1 += fitness[np.where(OriginalIndice == Ind[j])]\n",
        "        for th in range(len(Ind)):\n",
        "            TempFit[np.where(OriginalIndice == Ind[th])] = fitness[np.where(OriginalIndice == Ind[th])] / sum1\n",
        "        fitness = TempFit\n",
        "        if np.sum(co) >= N:\n",
        "            P = np.asarray(P)\n",
        "            P_fitness = np.asarray(P_fitness)\n",
        "            break\n",
        "\n",
        "    return P, P_fitness\n",
        "\n",
        "\n",
        "def MergeInChunk(P, P_fitness, sample, gamma, stdData):\n",
        "    \"\"\"Perform the Merge of TPCs witnin each data chunk\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc, dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked = []\n",
        "    unmarked = []\n",
        "    Com = []\n",
        "\n",
        "    # Num of TPCs\n",
        "    Nc = np.shape(P)[0]\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j != i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j, :] - P[i, :])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice <= Nc:\n",
        "                MinIndice = int(MinIndice)\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i, :]) / 2\n",
        "\n",
        "                X = np.reshape(X, (1, np.shape(P)[1]))\n",
        "\n",
        "                fitX = Fitness_Cal(sample, X, stdData, gamma)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 0.85 * min(fitN, fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i, MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k, 0]] >= P_fitness[Com[k, 1]]:\n",
        "            NewP.append(P[Com[k, 0], :])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k, 1], :])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n, :])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP, NewP_fitness\n",
        "\n",
        "\n",
        "def MergeOnline(P, P_fitness, P_summary, PreStd, sample, gamma, stdData):\n",
        "    \"\"\"Perform the Merge of Clusters Between Historical and New Clusters\n",
        "    \"\"\"\n",
        "    # Num of TPCs\n",
        "    [Nc, dim] = np.shape(P)\n",
        "    NewP = []\n",
        "    NewP_fitness = []\n",
        "    marked = []\n",
        "    unmarked = []\n",
        "    Com = []\n",
        "\n",
        "    for i in range(Nc):\n",
        "        MinDist = np.inf\n",
        "        MinIndice = 100000\n",
        "        if i not in marked:\n",
        "            for j in range(Nc):\n",
        "                if j != i and j not in marked:\n",
        "                    d = np.linalg.norm(P[j, :] - P[i, :])\n",
        "                    if d < MinDist:\n",
        "                        MinDist = d\n",
        "                        MinIndice = j\n",
        "            if MinIndice < Nc:\n",
        "\n",
        "                #                MinIndice = int(MinIndice)\n",
        "                Merge = True\n",
        "                Neighbor = P[MinIndice][:]\n",
        "                X = (Neighbor + P[i][:]) / 2\n",
        "                X = np.reshape(X, (1, np.shape(P)[1]))\n",
        "                RfitX = Fitness_Cal(sample, X, stdData, gamma)\n",
        "                fitX = fitness_update(P_Summary, X, RfitX, PreStd, gamma, stdData)\n",
        "                fitP = P_fitness[i]\n",
        "                fitN = P_fitness[MinIndice]\n",
        "                if fitX < 0.85 * min(fitN, fitP):\n",
        "                    Merge = False\n",
        "                if Merge:\n",
        "                    Com.append([i, MinIndice])\n",
        "                    marked.append(MinIndice)\n",
        "                    marked.append(i)\n",
        "                else:\n",
        "                    unmarked.append(i)\n",
        "    Com = np.asarray(Com)\n",
        "    # Number of Possible Merges:\n",
        "    Nm = np.shape(Com)[0]\n",
        "    for k in range(Nm):\n",
        "        if P_fitness[Com[k, 0]] >= P_fitness[Com[k, 1]]:\n",
        "            NewP.append(P[Com[k, 0]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 0]])\n",
        "        else:\n",
        "            NewP.append(P[Com[k, 1]][:])\n",
        "            NewP_fitness.append(P_fitness[Com[k, 1]])\n",
        "    # Add Unmerged TPCs to the NewP\n",
        "    for n in range(Nc):\n",
        "        if n not in Com:\n",
        "            NewP.append(P[n][:])\n",
        "            NewP_fitness.append(P_fitness[n])\n",
        "    NewP = np.asarray(NewP)\n",
        "    NewP_fitness = np.asarray(NewP_fitness)\n",
        "    return NewP, NewP_fitness\n",
        "\n",
        "\n",
        "def CE_InChunk(sample, P, P_fitness, stdData, gamma):\n",
        "    while 1:\n",
        "        HistP = P\n",
        "        #        HistPF = P_fitness\n",
        "        P, P_fitness = MergeInChunk(P, P_fitness, sample, gamma, stdData)\n",
        "        if np.shape(P)[0] == np.shape(HistP)[0]:\n",
        "            break\n",
        "    return P, P_fitness\n",
        "\n",
        "\n",
        "def CE_Online(sample, P_Summary, P, P_fitness, stdData, gamma, PreStd):\n",
        "    dim = np.shape(P)[1]\n",
        "\n",
        "    # Concatenate the historical and new clusters together\n",
        "    PC = np.concatenate([P_Summary[:, 0:dim], P])\n",
        "    RPF = Fitness_Cal(sample, PC, stdData, gamma)\n",
        "    PF = fitness_update(P_Summary, PC, RPF, PreStd, gamma, stdData)\n",
        "\n",
        "    while 1:\n",
        "        HistPC = PC\n",
        "        #        HistPF = PF\n",
        "        PC, PF = MergeOnline(PC, PF, P_Summary, PreStd, sample, gamma, stdData)\n",
        "        RPF = Fitness_Cal(sample, PC, stdData, gamma)\n",
        "        PF = fitness_update(P_Summary, PC, RPF, PreStd, gamma, stdData)\n",
        "        if np.shape(PC)[0] == np.shape(HistPC)[0]:\n",
        "            break\n",
        "    return PC, PF\n",
        "\n",
        "\n",
        "def ClusterValidation(sample, P):\n",
        "    while 1:\n",
        "        NewP = []\n",
        "        PreP = P\n",
        "        [R_d, RIndice] = Cluster_Assign(sample, P)\n",
        "\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            Temp = np.where(RIndice == i)\n",
        "            Temp = np.asarray(Temp)\n",
        "            if np.shape(Temp)[1] > 2:\n",
        "                NewP.append(P[i][:])\n",
        "        P = NewP\n",
        "        if np.shape(P)[0] == np.shape(PreP)[0]:\n",
        "            break\n",
        "    return np.asarray(P)\n",
        "\n",
        "\n",
        "def ClusterSummary(P, PF, P_Summary, sample):\n",
        "    dim = np.shape(sample)[1]\n",
        "    Rp = AverageDist(P, P_Summary, sample, dim)\n",
        "    P = np.asarray(P)\n",
        "    PF = [PF]\n",
        "\n",
        "    PF = np.asarray(PF)\n",
        "    Rp = np.reshape(Rp, (np.shape(P)[0], 1))\n",
        "    PCluster = np.concatenate([P, PF.T], axis=1)\n",
        "    PCluster = np.concatenate([PCluster, Rp], axis=1)\n",
        "\n",
        "    P_Summary = PCluster\n",
        "\n",
        "    return P_Summary\n",
        "\n",
        "\n",
        "def StoreInf(PF, PFS, PreStd, stdData):\n",
        "    PreStd.append(stdData)\n",
        "    PFS.append(PF)\n",
        "    return PreStd, PFS\n",
        "\n",
        "\n",
        "# --------------------Cluster Radius Computation and Update--------------------#\n",
        "def AverageDist(P, P_Summary, sample, dim):\n",
        "    P = P\n",
        "    # Obtain the assignment of clusters\n",
        "    [distance, indices] = Cluster_Assign(sample, P)\n",
        "    rad1 = []\n",
        "    # if the summary of clusters is not empty\n",
        "    if len(P_Summary) > 0:\n",
        "\n",
        "        PreP = P_Summary[:, 0:dim]  # Hstorical Cluster Center vector\n",
        "        PreR = P_Summary[:, dim + 1]\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            if np.shape(np.where(indices == i))[1] > 1:\n",
        "                SumD1 = 0\n",
        "                Count1 = 0\n",
        "                for j in range(np.shape(sample)[0]):\n",
        "                    if indices[j] == i:\n",
        "                        SumD1 += distance[j]\n",
        "                        Count1 += 1\n",
        "                rad1.append(SumD1 / Count1)\n",
        "            else:\n",
        "                C_d = []\n",
        "                for k in range(np.shape(PreP)[0]):\n",
        "                    C_d.append(np.linalg.norm(P[i][:] - PreP[k][:]))\n",
        "                CI = np.argmin(C_d)\n",
        "                rad1.append(PreR[CI])\n",
        "    elif not P_Summary:\n",
        "        for i in range(np.shape(P)[0]):\n",
        "            SumD1 = 0\n",
        "            Count1 = 0\n",
        "            for j in range(np.shape(sample)[0]):\n",
        "                if indices[j] == i:\n",
        "                    SumD1 += distance[j]\n",
        "                    Count1 += 1\n",
        "            rad1.append(SumD1 / Count1)\n",
        "    return np.asarray(rad1)\n",
        "\n",
        "\n",
        "def AssigntoPeaks(pop, pop_index, P, P_I, marked, radius, Dist):\n",
        "    temp = []\n",
        "    [N, L] = np.shape(pop)\n",
        "    for i in range(N):\n",
        "        distance = Dist[i, P_I]\n",
        "        if not np.any(marked == pop_index[i]):\n",
        "            if distance < radius:\n",
        "                temp.append(pop_index[i])\n",
        "    indices = temp\n",
        "    return indices\n",
        "\n",
        "\n",
        "def Cluster_Assign(sample, P):\n",
        "    # Number of samples\n",
        "    N = np.shape(sample)[0]\n",
        "    # Number of Clusters at t\n",
        "    Np = np.shape(P)[0]\n",
        "    MinDist = []\n",
        "    MinIndice = []\n",
        "    for i in range(N):\n",
        "        d = []\n",
        "        for j in range(Np):\n",
        "            d.append(np.linalg.norm(sample[i][:] - P[j][:]))\n",
        "        if len(d) <= 1:\n",
        "            tempD = d\n",
        "            tempI = 0\n",
        "        else:\n",
        "            tempD = np.min(d)\n",
        "            tempI = np.argmin(d)\n",
        "\n",
        "        MinDist.append(tempD)\n",
        "        MinIndice.append(tempI)\n",
        "    MinDist = np.asarray(MinDist)\n",
        "    MinIndice = np.asarray(MinIndice)\n",
        "    return MinDist, MinIndice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZwlS_ZIZhvm",
        "outputId": "f4cbd949-d6b7-40d4-c67b-6f30304c5e0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:472: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Data Chunk 0\n",
            "Processing Data Chunk 4\n",
            "Processing Data Chunk 8\n",
            "Processing Data Chunk 10\n",
            "Processing Data Chunk 13\n",
            "Processing Data Chunk 14\n",
            "Processing Data Chunk 15\n",
            "Processing Data Chunk 16\n",
            "Processing Data Chunk 17\n",
            "Processing Data Chunk 18\n",
            "Processing Data Chunk 23\n",
            "Processing Data Chunk 24\n",
            "Processing Data Chunk 25\n",
            "Processing Data Chunk 27\n",
            "Processing Data Chunk 32\n",
            "Processing Data Chunk 33\n",
            "Processing Data Chunk 35\n",
            "Processing Data Chunk 36\n",
            "Processing Data Chunk 37\n",
            "Processing Data Chunk 38\n",
            "Processing Data Chunk 39\n",
            "Processing Data Chunk 43\n",
            "Processing Data Chunk 47\n",
            "Processing Data Chunk 49\n",
            "Processing Data Chunk 50\n",
            "Processing Data Chunk 56\n",
            "Processing Data Chunk 57\n",
            "Processing Data Chunk 58\n",
            "Processing Data Chunk 59\n",
            "Processing Data Chunk 61\n",
            "Processing Data Chunk 64\n",
            "Processing Data Chunk 66\n",
            "Processing Data Chunk 67\n",
            "Processing Data Chunk 72\n"
          ]
        }
      ],
      "source": [
        "#---------------------------Main Function-------------------------#\n",
        "if __name__ == '__main__':\n",
        "  # X_train = X_train.astype('float32')\n",
        "  # X_test = X_test.astype('float32')\n",
        "\n",
        "  data = X_train.reshape(X_train.shape[0], 3072)\n",
        "  label = y_train\n",
        "  dim = np.shape(data)[1]\n",
        "  [BufferSize,P_Summary,T,PFS,PreStd] = ParamSpe(data)\n",
        "  T = int(T)\n",
        "  gammaHist = []\n",
        "  PFS = []\n",
        "  PreMu = []\n",
        "\n",
        "  for t in range(T):\n",
        "      if t < T-1:\n",
        "         sample = data[t*BufferSize:(t+1)*BufferSize,:]\n",
        "      else:\n",
        "          sample = data[t*BufferSize:np.shape(data)[0]]\n",
        "      if t==0:\n",
        "          AccSample = sample\n",
        "      else:\n",
        "          AccSample = np.concatenate([AccSample,sample])\n",
        "\n",
        "      [stdData,pop_index,pop,radius,PreMu,PreStd] = PopInitial(sample,PreMu,PreStd,BufferSize)\n",
        "      # print(stdData)\n",
        "      # Initialize the fitness vector\n",
        "      fitness = np.zeros((len(pop_index),1))\n",
        "      # Initialize the indices vector\n",
        "      indices = np.zeros((len(pop_index),1))\n",
        "      Dist = Distance_Cal(sample)\n",
        "      if PreStd:\n",
        "          if PreStd[len(PreStd)-1] > stdData:\n",
        "              P = P_Summary[:,0:dim]\n",
        "              localFit = Fitness_Cal(sample,P,stdData,gamma)\n",
        "              PF = fitness_update(P_Summary,P,localFit,PreStd,gamma,stdData)\n",
        "              P_Summary = ClusterSummary(P,PF,P_Summary,sample)\n",
        "              PFS.append(PF)\n",
        "              PreStd.append(stdData)\n",
        "              clustercenter = P\n",
        "              [Assign,clusterindex] = Cluster_Assign(AccSample,P)\n",
        "              continue\n",
        "      else:\n",
        "          gamma = CCA(sample,stdData,Dist)\n",
        "      gammaHist.append(gamma)\n",
        "      fitness = Fitness_Cal(sample,pop,stdData,gamma)\n",
        "      fitness = np.array(fitness)\n",
        "      P, P_fitness = TPC_Search(Dist,pop_index,pop,radius,fitness)\n",
        "      P, P_fitness = CE_InChunk(sample,P,P_fitness,stdData,gamma)\n",
        "      P_fitness = Fitness_Cal(sample,P,stdData,gamma)\n",
        "      P_fitness = fitness_update(P_Summary,P,P_fitness,PreStd,gamma,stdData)\n",
        "      print('Processing Data Chunk '+str(t))\n",
        "      if t == 0:\n",
        "          P = P\n",
        "          PF = np.asarray(P_fitness)\n",
        "      else:\n",
        "          P,P_fitness = CE_Online(sample,P_Summary,P,P_fitness,stdData,gamma,PreStd)\n",
        "          PF = np.asarray(P_fitness)\n",
        "      P_Summary = ClusterSummary(P,PF,P_Summary,sample)\n",
        "      PreStd,PFS = StoreInf(PF,PFS,PreStd,stdData)\n",
        "  # Clustering procedure finishes\n",
        "  [MinDist,ClusterIndice] = Cluster_Assign(AccSample,P)\n",
        "  # Start Active Learning\n",
        "  Nc = np.shape(P)[0]\n",
        "  num_S = round(np.shape(AccSample)[0] * 0.2)\n",
        "  FetchIndex = []\n",
        "  for i in range(np.shape(P)[0]):\n",
        "      tempcluster = np.where(ClusterIndice == (i))\n",
        "      d1 = []\n",
        "      for j in range(len(tempcluster[0])):\n",
        "          d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "      fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "      sortIndex1 = np.argsort(d1)\n",
        "      fet1 = tempcluster[0][sortIndex1[:round(fetchSize * 0.5)]]\n",
        "      fet2 = tempcluster[0][sortIndex1[-round(fetchSize * 0.5):]]\n",
        "      fet2 = fet2.astype(int)\n",
        "      fet1 = fet1.astype(int)\n",
        "      FetchIdex = np.append(FetchIndex, fet2)\n",
        "      FetchIndex = np.append(FetchIndex, fet1)\n",
        "  FetchIndex = FetchIndex.astype(int)\n",
        "  queried_data = AccSample[FetchIndex][:]\n",
        "  queried_label1 = label[FetchIndex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q_gVq8ykshq-"
      },
      "outputs": [],
      "source": [
        "# Nc = np.shape(P)[0]\n",
        "# num_S = round(np.shape(AccSample)[0] * 0.4)\n",
        "# FetchIndex = []\n",
        "# for i in range(np.shape(P)[0]):\n",
        "#   tempcluster = np.where(ClusterIndice == (i))\n",
        "#   d1 = []\n",
        "#   for j in range(len(tempcluster[0])):\n",
        "#         d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "#   fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "#   sortIndex1 = np.argsort(d1)\n",
        "#   fet1 = tempcluster[0][sortIndex1[:round(fetchSize * 0.5)]]\n",
        "#   fet2 = tempcluster[0][sortIndex1[-round(fetchSize * 0.5):]]\n",
        "#   fet2 = fet2.astype(int)\n",
        "#   fet1 = fet1.astype(int)\n",
        "#   FetchIdex = np.append(FetchIndex, fet2)\n",
        "#   FetchIndex = np.append(FetchIndex, fet1)\n",
        "# FetchIndex = FetchIndex.astype(int)\n",
        "# queried_data = AccSample[FetchIndex][:]\n",
        "# queried_label1 = label[FetchIndex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YBVHos65xEzj"
      },
      "outputs": [],
      "source": [
        "# # Start Active Learning\n",
        "# def compute_radius(MinDist,ClusterIndice):\n",
        "#     cluster = np.unique(ClusterIndice)\n",
        "#     nc = len(cluster)\n",
        "#     cluster_rad = []\n",
        "#     for i in range(nc):\n",
        "#         currentcluster = np.where(ClusterIndice==cluster[i])[0]\n",
        "#         cluster_rad.append(np.mean(MinDist[currentcluster]))\n",
        "#     return cluster_rad\n",
        "# sample_size = np.shape(AccSample)[0]\n",
        "# num_S = round(0.2 * sample_size)\n",
        "# cluster_radius = compute_radius(MinDist, ClusterIndice)\n",
        "# delta = np.mean(cluster_radius)\n",
        "# sigma = np.std(cluster_radius)\n",
        "# dense_idx = np.where(cluster_radius <= abs(delta - sigma))[0]\n",
        "# sparese_idx = np.where(cluster_radius > abs(delta - sigma))[0]\n",
        "\n",
        "# FetchIndex = []\n",
        "# UnlabeledIndex = []\n",
        "# InterDist = squareform(pdist(P))\n",
        "\n",
        "# FetchIndex = []\n",
        "# UnlabeledIndex = []\n",
        "# InterDist = squareform(pdist(P))\n",
        "\n",
        "# for i in range(np.shape(P)[0]):\n",
        "#   tempcluster = np.where(ClusterIndice == (i))\n",
        "#   d1 = []\n",
        "#   if i in dense_idx:\n",
        "#     for j in range(len(tempcluster[0])):\n",
        "#       d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "#       fetchSize = round(num_S * cluster_radius[i] / (1+np.sum(cluster_radius)))\n",
        "\n",
        "#       fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "#       sortIndex1 = np.argsort(d1)\n",
        "#       fet1 = tempcluster[0][sortIndex1[:int(round(fetchSize * 1))]]\n",
        "#       fet1 = fet1.astype(int)\n",
        "#       fet2 = []\n",
        "#       FetchIndex = np.append(FetchIndex, fet1)\n",
        "#       FetchIndex = np.append(FetchIndex, fet2)\n",
        "#       UnlabeledIndex = np.append(UnlabeledIndex,tempcluster[0][sortIndex1[int(round(fetchSize * 1)):round(len(d1))]])\n",
        "#   else:\n",
        "#       temp_interdist = InterDist[i, :]\n",
        "#       temp_rank = np.argsort(temp_interdist)\n",
        "#       temp_neigh1 = P[temp_rank[0], :]\n",
        "#       temp_neigh2 = P[temp_rank[1], :]\n",
        "#       d1 = []\n",
        "#       for j in range(len(tempcluster[0])):\n",
        "#         d1.append(np.linalg.norm(AccSample[tempcluster[0][j], :] - P[i, :]))\n",
        "#       fetchSize = round(num_S * cluster_radius[i] / np.sum(cluster_radius))\n",
        "#       fetchSize = num_S * len(d1) / np.shape(AccSample)[0]\n",
        "#       sortIndex1 = np.argsort(d1)\n",
        "#       fet1 = tempcluster[0][sortIndex1[:int(round(fetchSize * 0.5))]]\n",
        "#       fet1 = fet1.astype(int)\n",
        "#       fil_index = sortIndex1[-int(round(len(d1) / 2)):]\n",
        "#       d2 = []\n",
        "#       for k in range(len(fil_index)):\n",
        "#           temp_d1 = np.linalg.norm(AccSample[tempcluster[0][fil_index[k]], :] - temp_neigh1)\n",
        "#           temp_d2 = np.linalg.norm(AccSample[tempcluster[0][fil_index[k]], :] - temp_neigh2)\n",
        "#           temp_ratio1 = max(temp_d1, temp_d2) / min(temp_d1, temp_d2)\n",
        "#           temp_ratio2 = (temp_d1 + temp_d2) / np.linalg.norm(temp_neigh1 - temp_neigh2)\n",
        "#           d2.append(temp_ratio1)\n",
        "#       sortIndex2 = np.argsort(d2)\n",
        "#       candidate_fet2 = fil_index[sortIndex2[:int(round(fetchSize * 0.8))]]\n",
        "#       sum_dist = []\n",
        "#       for ii in range(len(candidate_fet2)):\n",
        "#         candidate_d1 = np.linalg.norm(AccSample[tempcluster[0][candidate_fet2[ii]], :] - temp_neigh1)\n",
        "#         candidate_d2 = np.linalg.norm(AccSample[tempcluster[0][candidate_fet2[ii]], :] - temp_neigh1)\n",
        "#         sum_dist.append(candidate_d1 + candidate_d2)\n",
        "\n",
        "#       sortIndex3 = np.argsort(sum_dist)\n",
        "#       fet2 = tempcluster[0][candidate_fet2[sortIndex3[:int(round(fetchSize * 0.5))]]]\n",
        "#       fet2 = fet2.astype(int)\n",
        "\n",
        "#       FetchIndex = np.append(FetchIndex, fet1)\n",
        "#       FetchIndex = np.append(FetchIndex, fet2)\n",
        "#       UnlabeledIndex = np.append(UnlabeledIndex, tempcluster[0][sortIndex1[int(round(fetchSize * 0.5)):round(len(d1) / 2)]])\n",
        "#       UnlabeledIndex = np.append(UnlabeledIndex, tempcluster[0][fil_index[sortIndex2[int(round(fetchSize * 0.5)):len(d2)]]])\n",
        "# FetchIndex = FetchIndex.astype(int)\n",
        "# UnlabeledIndex = UnlabeledIndex.astype(int)\n",
        "# queried_data = AccSample[FetchIndex][:]\n",
        "# queried_label1 = label[FetchIndex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w_Um9VblmpKL",
        "outputId": "bbb91a13-fbef-42d7-a53b-919aa5bc0b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of clusters:  (67, 3072)\n",
            "Number of queried labels:  7324\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of clusters: \",np.shape(P))\n",
        "print(\"Number of queried labels: \",len(label[FetchIndex]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7v7-FPcH_q-g",
        "outputId": "70e4c548-5a15-4a79-95b2-9221bd2ef883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7324, 1)\n"
          ]
        }
      ],
      "source": [
        "[nq, lq] = np.shape(queried_data)\n",
        "queried_label1 = label[FetchIndex]\n",
        "print(label[FetchIndex].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Ah0bZCt80ZI"
      },
      "outputs": [],
      "source": [
        "# Build CNN using active learned samples\n",
        "#[nq, lq] = np.shape(queried_data)\n",
        "nt = X_test.shape[0]\n",
        "queried_data1 = np.reshape(queried_data, (nq,32,32,3))\n",
        "queried_label1 = np.reshape(queried_label1,(nq, 1))\n",
        "testing_data1 = np.reshape(X_test, (nt,32,32,3))\n",
        "testing_label1 = np.reshape(y_test, (nt,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lotFInpZ8rfA"
      },
      "outputs": [],
      "source": [
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "# model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8umYIOPJ9eC5"
      },
      "outputs": [],
      "source": [
        "# test_X, val_X, test_Y, val_Y = train_test_split(testing_data1, testing_label1, test_size = 0.2)\n",
        "# from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6z8qXC-F9nwJ"
      },
      "outputs": [],
      "source": [
        "# opt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# # history = model.fit(queried_data1, queried_label1, epochs=60, batch_size=64)\n",
        "# history = model.fit(queried_data1, queried_label1, epochs=20, batch_size=128,\n",
        "#                           validation_data=(val_X, val_Y), verbose=1)\n",
        "# score = model.evaluate(testing_data1, testing_label1, verbose=0)\n",
        "# print(\"Testing accuracy:\", score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2lAIKrgjeUW7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.3),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88YUGoeJyBZ3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xgVIsMZpe0SK",
        "outputId": "84ad3b71-0012-41fc-d100-b8dda932432d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "labels of y_train are [0 1 2 3 4 5 6 7 8 9]\n",
            "labels of y_test are [0 1 2 3 4 5 6 7 8 9]\n",
            "Epoch 1/10\n",
            "229/229 [==============================] - 28s 117ms/step - loss: 2.2320 - accuracy: 0.1948\n",
            "Epoch 2/10\n",
            "229/229 [==============================] - 27s 117ms/step - loss: 2.2254 - accuracy: 0.1958\n",
            "Epoch 3/10\n",
            "229/229 [==============================] - 27s 119ms/step - loss: 2.0462 - accuracy: 0.2779\n",
            "Epoch 4/10\n",
            "229/229 [==============================] - 28s 120ms/step - loss: 1.3663 - accuracy: 0.5470\n",
            "Epoch 5/10\n",
            "229/229 [==============================] - 27s 120ms/step - loss: 1.0149 - accuracy: 0.6697\n",
            "Epoch 6/10\n",
            "229/229 [==============================] - 27s 118ms/step - loss: 0.8620 - accuracy: 0.7219\n",
            "Epoch 7/10\n",
            "229/229 [==============================] - 27s 120ms/step - loss: 0.7355 - accuracy: 0.7627\n",
            "Epoch 8/10\n",
            "229/229 [==============================] - 29s 128ms/step - loss: 0.6551 - accuracy: 0.7904\n",
            "Epoch 9/10\n",
            "229/229 [==============================] - 27s 117ms/step - loss: 0.5877 - accuracy: 0.8073\n",
            "Epoch 10/10\n",
            "229/229 [==============================] - 27s 118ms/step - loss: 0.5189 - accuracy: 0.8338\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11e5079f10>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"labels of y_train are\", np.unique(y_train[:]))\n",
        "print(\"labels of y_test are\", np.unique(y_test[:]))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(queried_data1, queried_label1, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOEkJladrIOD"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Model accuracy on test data is: {:6.3f}%\".format(100 * acc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP190OjjIM/K/qA4ZskFDpi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}